<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
  Neural Networks &ndash; cl-waffe
</title>
    <link rel="stylesheet" href="static/style.css"/>
    
  <link rel="stylesheet" href="static/highlight.css"/>
  <script src="static/highlight.js"></script>
  <style>
   /* Highlight the current top-level TOC item, and hide the TOC of all other items */

   .toc a[data-node="neural-networks"] {
       /*color: #AD3108;*/
   }

   .toc ol {
       display: none;
   }

   .toc li a[data-node="neural-networks"] {
       font-weight: bold;
   }

   .toc li a[data-node="neural-networks"] + ol {
       display: block;
   }

   .toc li a[data-node="neural-networks"] + ol li {
       margin-left: 10px;
   }
  </style>

  </head>
  <body>
    
  <h1 class="doc-title">cl-waffe</h1>
  <article id="article" data-section="neural-networks">
    <aside>
      <ol class="toc"><li><a href="overview.html" data-node="overview">Overview</a><ol><li><a href="overview.html#welcome-to-cl-waffe!" data-node="welcome-to-cl-waffe!">Welcome to cl-waffe!</a></li><li><a href="overview.html#problems" data-node="problems">Problems</a></li><li><a href="overview.html#sections" data-node="sections">Sections</a></li><li><a href="overview.html#pull-requests" data-node="pull-requests">Pull Requests</a></li><li><a href="overview.html#contacts" data-node="contacts">Contacts</a></li><li><a href="overview.html#lla-setting" data-node="lla-setting">LLA Setting</a></li><li><a href="overview.html#when-memory-exhausted" data-node="when-memory-exhausted">When Memory Exhausted</a></li></ol></li><li><a href="mnist-tutorial.html" data-node="mnist-tutorial">MNIST Tutorial</a><ol><li><a href="mnist-tutorial.html#first" data-node="first">First</a></li><li><a href="mnist-tutorial.html#define-your-model" data-node="define-your-model">Define Your Model</a></li><li><a href="mnist-tutorial.html#define-your-dataset" data-node="define-your-dataset">Define Your Dataset</a><ol><li><a href="cl-waffe's-dataset--waffedataset.html" data-node="cl-waffe's-dataset--waffedataset">cl-waffe's Dataset: WaffeDataSet</a></li></ol></li><li><a href="mnist-tutorial.html#train-your-model" data-node="train-your-model">Train Your Model</a></li></ol></li><li><a href="using-tensor.html" data-node="using-tensor">Using Tensor</a><ol><li><a href="using-tensor.html#basic-tensor-operations" data-node="basic-tensor-operations">Basic Tensor Operations</a></li><li><a href="using-tensor.html#building-computation-nodes" data-node="building-computation-nodes">Building Computation Nodes</a><ol><li><a href="using-tensor.html#construct-tensors" data-node="construct-tensors">Construct Tensors</a><ol><li><a href="using-tensor.html#constants" data-node="constants">Constants</a></li><li><a href="using-tensor.html#parameter-tensors" data-node="parameter-tensors">Parameter Tensors</a></li><li><a href="using-tensor.html#sysconst" data-node="sysconst">Sysconst</a></li></ol></li></ol></li><li><a href="using-tensor.html#accessing-tensor" data-node="accessing-tensor">Accessing Tensor</a></li><li><a href="using-tensor.html#backward-and-predicting-mode" data-node="backward-and-predicting-mode">backward and predicting mode</a></li><li><a href="using-tensor.html#calling-forward-of-cl-waffe's-objects" data-node="calling-forward-of-cl-waffe's-objects">Calling Forward of cl-waffe's objects</a></li><li><a href="using-tensor.html#displaying-tensors" data-node="displaying-tensors">Displaying Tensors</a></li><li><a href="using-tensor.html#types" data-node="types">Types</a></li><li><a href="using-tensor.html#lazy-evaluation" data-node="lazy-evaluation">Lazy evaluation</a></li><li><a href="using-tensor.html#broadcasting" data-node="broadcasting">Broadcasting</a></li><li><a href="using-tensor.html#jit" data-node="jit">JIT</a></li><li><a href="using-tensor.html#tracing" data-node="tracing">Tracing</a></li><li><a href="using-tensor.html#compute-tensors-in-a-destructive-way" data-node="compute-tensors-in-a-destructive-way">Compute tensors in a destructive way</a><ol><li><a href="using-tensor.html#creating-destructive-operations" data-node="creating-destructive-operations">Creating Destructive Operations</a></li></ol></li></ol></li><li><a href="extend-library.html" data-node="extend-library">Extend library</a><ol><li><a href="extend-library.html#defmodel" data-node="defmodel">defmodel</a><ol><li><a href="extend-library.html#initialize-and-call-model" data-node="initialize-and-call-model">Initialize and call model</a></li><li><a href="extend-library.html#clos-style" data-node="clos-style">CLOS Style</a></li></ol></li><li><a href="extend-library.html#deftrainer" data-node="deftrainer">deftrainer</a></li><li><a href="extend-library.html#defoptimizer" data-node="defoptimizer">defoptimizer</a></li><li><a href="extend-library.html#defnode" data-node="defnode">defnode</a></li><li><a href="extend-library.html#defdataset" data-node="defdataset">defdataset</a></li></ol></li><li><a href="cl-waffe.html" data-node="cl-waffe">cl-waffe</a><ol><li><a href="cl-waffe.html#package--cl-waffe" data-node="package--cl-waffe">Package :cl-waffe</a></li><li><a href="cl-waffe.html#0-sections" data-node="0-sections">Sections</a></li><li><a href="cl-waffe.html#model-and-node" data-node="model-and-node">Model And Node</a></li><li><a href="cl-waffe.html#1-defmodel" data-node="1-defmodel">defmodel</a></li><li><a href="cl-waffe.html#2-defnode" data-node="2-defnode">defnode</a></li><li><a href="cl-waffe.html#trainer" data-node="trainer">Trainer</a></li><li><a href="cl-waffe.html#3-deftrainer" data-node="3-deftrainer">deftrainer</a></li><li><a href="cl-waffe.html#datasets" data-node="datasets">Datasets</a></li><li><a href="cl-waffe.html#4-defdataset" data-node="4-defdataset">defdataset</a></li><li><a href="cl-waffe.html#optimizer" data-node="optimizer">optimizer</a></li><li><a href="cl-waffe.html#5-defoptimizer" data-node="5-defoptimizer">defoptimizer</a></li><li><a href="cl-waffe.html#documentation-template" data-node="documentation-template">Documentation Template</a></li></ol></li><li><a href="cl-waffe.nn.html" data-node="cl-waffe.nn">cl-waffe.nn</a><ol><li><a href="cl-waffe.nn.html#exported" data-node="exported">Exported</a></li></ol></li><li><a href="cl-waffe.optimizers.html" data-node="cl-waffe.optimizers">cl-waffe.optimizers</a><ol><li><a href="cl-waffe.optimizers.html#6-sections" data-node="6-sections">Sections</a></li><li><a href="cl-waffe.optimizers.html#training-models-with-optimizer" data-node="training-models-with-optimizer">Training Models With Optimizer</a></li></ol></li><li><a href="cl-waffe.io.html" data-node="cl-waffe.io">cl-waffe.io</a><ol><li><a href="cl-waffe.io.html#7-exported" data-node="7-exported">Exported</a></li></ol></li><li><a href="cl-waffe.caches.html" data-node="cl-waffe.caches">cl-waffe.caches</a><ol><li><a href="cl-waffe.caches.html#8-exported" data-node="8-exported">Exported</a></li></ol></li><li><a href="operators.html" data-node="operators">Operators</a><ol><li><a href="operators.html#!shape" data-node="!shape">!shape</a></li><li><a href="operators.html#!dims" data-node="!dims">!dims</a></li><li><a href="operators.html#!size" data-node="!size">!size</a></li><li><a href="operators.html#!zeros" data-node="!zeros">!zeros</a></li><li><a href="operators.html#!ones" data-node="!ones">!ones</a></li><li><a href="operators.html#!fill" data-node="!fill">!fill</a></li><li><a href="operators.html#!arange" data-node="!arange">!arange</a><ol><li><a href="(-!arange-stop-).html" data-node="(-!arange-stop-)">(!arange stop)</a></li><li><a href="(-!arange-start-stop-).html" data-node="(-!arange-start-stop-)">(!arange start stop)</a></li><li><a href="(-!arange-start-stop-step-).html" data-node="(-!arange-start-stop-step-)">(!arange start stop step)</a></li></ol></li><li><a href="operators.html#!random" data-node="!random">!random</a><ol><li><a href="when-limit=fixnum.html" data-node="when-limit=fixnum">When limit=fixnum</a></li><li><a href="when-limit=single-float.html" data-node="when-limit=single-float">When limit=single-float</a></li><li><a href="when-limit=-(-cons-single-float1-single-float2-).html" data-node="when-limit=-(-cons-single-float1-single-float2-)">When limit=(cons single-float1 single-float2)</a></li></ol></li><li><a href="operators.html#!random-with" data-node="!random-with">!random-with</a></li><li><a href="operators.html#!init-with" data-node="!init-with">!init-with</a></li><li><a href="operators.html#!normal" data-node="!normal">!normal</a></li><li><a href="operators.html#!randn" data-node="!randn">!randn</a></li><li><a href="operators.html#!beta" data-node="!beta">!beta</a></li><li><a href="operators.html#!gamma" data-node="!gamma">!gamma</a></li><li><a href="operators.html#!chisquare" data-node="!chisquare">!chisquare</a></li><li><a href="operators.html#!bernoulli" data-node="!bernoulli">!bernoulli</a></li><li><a href="operators.html#!binomial" data-node="!binomial">!binomial</a></li><li><a href="operators.html#9-!shape" data-node="9-!shape">!shape</a></li><li><a href="operators.html#10-!dims" data-node="10-!dims">!dims</a></li><li><a href="operators.html#11-!size" data-node="11-!size">!size</a></li><li><a href="operators.html#!zeros-like" data-node="!zeros-like">!zeros-like</a></li><li><a href="operators.html#!ones-like" data-node="!ones-like">!ones-like</a></li><li><a href="operators.html#!full-like" data-node="!full-like">!full-like</a></li><li><a href="operators.html#!add" data-node="!add">!add</a><ol><li><a href="examples.html" data-node="examples">Examples</a></li></ol></li><li><a href="operators.html#!sub" data-node="!sub">!sub</a><ol><li><a href="12-examples.html" data-node="12-examples">Examples</a></li></ol></li><li><a href="operators.html#!mul" data-node="!mul">!mul</a><ol><li><a href="13-examples.html" data-node="13-examples">Examples</a></li></ol></li><li><a href="operators.html#!div" data-node="!div">!div</a><ol><li><a href="14-examples.html" data-node="14-examples">Examples</a></li></ol></li><li><a href="operators.html#!dot" data-node="!dot">!dot</a><ol><li><a href="example.html" data-node="example">Example</a></li></ol></li><li><a href="operators.html#!sum" data-node="!sum">!sum</a><ol><li><a href="arguments.html" data-node="arguments">arguments</a></li><li><a href="15-example.html" data-node="15-example">Example</a></li></ol></li><li><a href="operators.html#!mean" data-node="!mean">!mean</a><ol><li><a href="16-example.html" data-node="16-example">Example</a></li></ol></li><li><a href="operators.html#!exp" data-node="!exp">!exp</a><ol><li><a href="17-example.html" data-node="17-example">Example</a></li></ol></li><li><a href="operators.html#!pow" data-node="!pow">!pow</a><ol><li><a href="18-example.html" data-node="18-example">Example</a></li></ol></li><li><a href="operators.html#!sqrt" data-node="!sqrt">!sqrt</a><ol><li><a href="19-example.html" data-node="19-example">Example</a></li></ol></li><li><a href="operators.html#!log" data-node="!log">!log</a><ol><li><a href="20-example.html" data-node="20-example">Example</a></li></ol></li><li><a href="operators.html#!sin" data-node="!sin">!sin</a><ol><li><a href="21-example.html" data-node="21-example">Example</a></li></ol></li><li><a href="operators.html#!cos" data-node="!cos">!cos</a><ol><li><a href="22-example.html" data-node="22-example">Example</a></li></ol></li><li><a href="operators.html#!tan" data-node="!tan">!tan</a><ol><li><a href="23-example.html" data-node="23-example">Example</a></li></ol></li><li><a href="operators.html#!asin" data-node="!asin">!asin</a></li><li><a href="operators.html#!acos" data-node="!acos">!acos</a></li><li><a href="operators.html#!atan" data-node="!atan">!atan</a></li><li><a href="operators.html#!sinh" data-node="!sinh">!sinh</a><ol><li><a href="24-example.html" data-node="24-example">Example</a></li></ol></li><li><a href="operators.html#!cosh" data-node="!cosh">!cosh</a><ol><li><a href="25-example.html" data-node="25-example">Example</a></li></ol></li><li><a href="operators.html#!tanh" data-node="!tanh">!tanh</a></li><li><a href="operators.html#!asinh" data-node="!asinh">!asinh</a></li><li><a href="operators.html#!acosh" data-node="!acosh">!acosh</a></li><li><a href="operators.html#!atanh" data-node="!atanh">!atanh</a></li><li><a href="operators.html#!matmul" data-node="!matmul">!matmul</a></li><li><a href="operators.html#!unsqueeze" data-node="!unsqueeze">!unsqueeze</a><ol><li><a href="26-example.html" data-node="26-example">Example</a></li></ol></li><li><a href="operators.html#!squeeze" data-node="!squeeze">!squeeze</a><ol><li><a href="27-example.html" data-node="27-example">Example</a></li></ol></li><li><a href="operators.html#!transpose" data-node="!transpose">!transpose</a><ol><li><a href="28-example.html" data-node="28-example">Example</a></li></ol></li><li><a href="operators.html#!transpose1" data-node="!transpose1">!transpose1</a><ol><li><a href="29-example.html" data-node="29-example">Example</a></li></ol></li><li><a href="operators.html#!repeats" data-node="!repeats">!repeats</a><ol><li><a href="30-example.html" data-node="30-example">Example</a></li></ol></li><li><a href="operators.html#!reshape" data-node="!reshape">!reshape</a><ol><li><a href="31-example.html" data-node="31-example">Example</a></li></ol></li><li><a href="operators.html#!abs" data-node="!abs">!abs</a></li><li><a href="operators.html#!where" data-node="!where">!where</a><ol><li><a href="32-example.html" data-node="32-example">Example</a></li></ol></li><li><a href="operators.html#!index" data-node="!index">!index</a></li><li><a href="operators.html#!filter" data-node="!filter">!filter</a></li><li><a href="operators.html#!argmax" data-node="!argmax">!argmax</a><ol><li><a href="33-example.html" data-node="33-example">Example</a></li></ol></li><li><a href="operators.html#!argmin" data-node="!argmin">!argmin</a><ol><li><a href="34-example.html" data-node="34-example">Example</a></li></ol></li><li><a href="operators.html#!<=" data-node="!<=">!&lt;=</a></li><li><a href="operators.html#!>=" data-node="!>=">!&gt;=</a></li><li><a href="operators.html#!einsum" data-node="!einsum">!einsum</a></li><li><a href="operators.html#!ravel" data-node="!ravel">!ravel</a></li><li><a href="operators.html#!flatten" data-node="!flatten">!flatten</a></li><li><a href="operators.html#!aref" data-node="!aref">!aref</a></li><li><a href="operators.html#!dotensors" data-node="!dotensors">!dotensors</a></li><li><a href="operators.html#!set-batch" data-node="!set-batch">!set-batch</a></li><li><a href="operators.html#!softmax" data-node="!softmax">!softmax</a></li><li><a href="operators.html#!sigmoid" data-node="!sigmoid">!sigmoid</a></li><li><a href="operators.html#!relu" data-node="!relu">!relu</a></li><li><a href="operators.html#!gelu" data-node="!gelu">!gelu</a></li><li><a href="operators.html#!leakey-relu" data-node="!leakey-relu">!leakey-relu</a></li><li><a href="operators.html#!swish" data-node="!swish">!swish</a></li></ol></li><li><a href="neural-networks.html" data-node="neural-networks">Neural Networks</a><ol><li><a href="neural-networks.html#model-list" data-node="model-list">model-list</a><ol><li><a href="neural-networks.html#parameters" data-node="parameters">Parameters</a></li><li><a href="neural-networks.html#forward" data-node="forward">Forward</a></li><li><a href="neural-networks.html#35-example" data-node="35-example">Example</a></li></ol></li><li><a href="neural-networks.html#linearlayer" data-node="linearlayer">Linearlayer</a><ol><li><a href="neural-networks.html#36-parameters" data-node="36-parameters">Parameters</a></li><li><a href="neural-networks.html#shape" data-node="shape">Shape</a></li><li><a href="neural-networks.html#37-forward" data-node="37-forward">Forward</a></li><li><a href="neural-networks.html#38-example" data-node="38-example">Example</a></li></ol></li><li><a href="neural-networks.html#denselayer" data-node="denselayer">DenseLayer</a><ol><li><a href="neural-networks.html#39-parameters" data-node="39-parameters">Parameters</a></li><li><a href="neural-networks.html#40-shape" data-node="40-shape">Shape</a></li><li><a href="neural-networks.html#41-forward" data-node="41-forward">Forward</a></li><li><a href="neural-networks.html#42-example" data-node="42-example">Example</a></li></ol></li><li><a href="neural-networks.html#dropout" data-node="dropout">Dropout</a><ol><li><a href="neural-networks.html#43-parameters" data-node="43-parameters">Parameters</a></li><li><a href="neural-networks.html#44-shape" data-node="44-shape">Shape</a></li><li><a href="neural-networks.html#45-forward" data-node="45-forward">Forward</a></li></ol></li><li><a href="neural-networks.html#batchnorm2d" data-node="batchnorm2d">BatchNorm2d</a><ol><li><a href="neural-networks.html#46-parameters" data-node="46-parameters">Parameters</a></li><li><a href="neural-networks.html#47-shape" data-node="47-shape">Shape</a></li><li><a href="neural-networks.html#48-example" data-node="48-example">Example</a></li></ol></li><li><a href="neural-networks.html#layernorm" data-node="layernorm">LayerNorm</a></li><li><a href="neural-networks.html#embedding" data-node="embedding">Embedding</a><ol><li><a href="neural-networks.html#parameter" data-node="parameter">Parameter</a></li><li><a href="neural-networks.html#49-shape" data-node="49-shape">Shape</a></li><li><a href="neural-networks.html#50-example" data-node="50-example">Example</a></li></ol></li><li><a href="neural-networks.html#rnn" data-node="rnn">RNN</a><ol><li><a href="neural-networks.html#51-parameters" data-node="51-parameters">Parameters</a></li><li><a href="neural-networks.html#52-shape" data-node="52-shape">Shape</a></li><li><a href="neural-networks.html#53-example" data-node="53-example">Example</a></li></ol></li><li><a href="neural-networks.html#lstm" data-node="lstm">LSTM</a></li><li><a href="neural-networks.html#gru" data-node="gru">GRU</a></li><li><a href="neural-networks.html#maxpooling" data-node="maxpooling">MaxPooling</a></li><li><a href="neural-networks.html#avgpooling" data-node="avgpooling">AvgPooling</a></li><li><a href="neural-networks.html#conv1d" data-node="conv1d">Conv1D</a></li><li><a href="neural-networks.html#conv2d" data-node="conv2d">Conv2D</a></li><li><a href="neural-networks.html#transformer" data-node="transformer">Transformer</a></li><li><a href="neural-networks.html#transformerencoderlayer" data-node="transformerencoderlayer">TransformerEncoderLayer</a></li><li><a href="neural-networks.html#transformerdecoderlayer" data-node="transformerdecoderlayer">TransformerDecoderLayer</a></li><li><a href="neural-networks.html#crossentropy" data-node="crossentropy">CrossEntropy</a></li><li><a href="neural-networks.html#softmaxcrossentropy" data-node="softmaxcrossentropy">SoftMaxCrossEntropy</a></li><li><a href="neural-networks.html#mse" data-node="mse">MSE</a></li><li><a href="neural-networks.html#l1norm" data-node="l1norm">L1Norm</a></li><li><a href="neural-networks.html#l2norm" data-node="l2norm">L2Norm</a></li><li><a href="neural-networks.html#binarycrossentropy" data-node="binarycrossentropy">BinaryCrossEntropy</a></li><li><a href="neural-networks.html#kldivloss" data-node="kldivloss">KLdivLoss</a></li><li><a href="neural-networks.html#cosinesimilarity" data-node="cosinesimilarity">CosineSimilarity</a></li></ol></li><li><a href="optimizers.html" data-node="optimizers">Optimizers</a><ol><li><a href="sgd.html" data-node="sgd">SGD</a><ol><li><a href="cl-waffe's-optimizer--sgd.html" data-node="cl-waffe's-optimizer--sgd">cl-waffe's Optimizer: SGD</a></li></ol></li><li><a href="momentum.html" data-node="momentum">Momentum</a><ol><li><a href="cl-waffe's-optimizer--momentum.html" data-node="cl-waffe's-optimizer--momentum">cl-waffe's Optimizer: Momentum</a></li></ol></li><li><a href="adagrad.html" data-node="adagrad">AdaGrad</a><ol><li><a href="cl-waffe's-optimizer--adagrad.html" data-node="cl-waffe's-optimizer--adagrad">cl-waffe's Optimizer: AdaGrad</a></li></ol></li><li><a href="rmsprop.html" data-node="rmsprop">RMSProp</a><ol><li><a href="cl-waffe's-optimizer--rmsprop.html" data-node="cl-waffe's-optimizer--rmsprop">cl-waffe's Optimizer: RMSProp</a></li></ol></li><li><a href="adam.html" data-node="adam">Adam</a><ol><li><a href="cl-waffe's-optimizer--adam.html" data-node="cl-waffe's-optimizer--adam">cl-waffe's Optimizer: Adam</a></li></ol></li><li><a href="adamw.html" data-node="adamw">AdamW</a></li><li><a href="radam.html" data-node="radam">RAdam</a></li></ol></li></ol>
    </aside>
    <main class="codex-section">
      <header>
        <h2 class="section-title">Neural Networks</h2>
      </header>
      <div class="content">
        <h1 id="model-list">model-list</h1><p>
Holds submodules in a list.</p><p>Model-List it contains are properly tracked by <code class="codex-param">find-variables</code>.</p><p>Note: This Layer is exported from Package <code class="codex-param">cl-waffe</code>.
</p><h2 id="parameters">Parameters</h2><p>
</p><pre><code class="lisp">(model-list list)
</code></pre><p>
</p><dl><dt>list (list)</dt><dd>an list of models</dd></dl><p>This model can also be created by <code class="codex-param">mlist</code></p><pre><code class="lisp">(mlist models) ; -&gt; [Model: MODEL-LIST]
</code></pre><p>
</p><h2 id="forward">Forward</h2><p>
</p><pre><code class="lisp">(call (Model-List) index &amp;rest args)
</code></pre><p>Note that <code class="codex-param">index</code> must be waffetensor.</p><p>To avoid this, <code class="codex-param">mth</code> is available.</p><pre><code class="lisp">(call (mth 0 (Model-List)) &amp;rest args)
</code></pre><dl><dt>index (waffetensor of which data is fixnum)</dt><dd>an index of models</dd><dt>args (list)</dt><dd>arguments for index-th model</dd></dl><p>
</p><h2 id="35-example">Example</h2>
<pre><code class="lisp">(setq models (Model-List (list (linearlayer 10 1)(linearlayer 10 1))))
(call models (const 0)(!randn `(10 10)))
(call (mth 0 models)(!randn `(10 10)))
</code></pre>
<p>
</p><h1 id="linearlayer">Linearlayer</h1><p>
Applies a linear transformation to the incoming data: <code>(setq y (!add (!matmul x weight) bias))</code></p><h2 id="36-parameters">Parameters</h2>
<pre><code class="lisp">(LinearLayer in-features out-features &amp;optional (bias T))
</code></pre>
<dl><dt>in-features (fixnum)</dt><dd>size of each input sample</dd><dt>out-features (fixnum)</dt><dd>size of each output sample</dd><dt>bias (boolean)</dt><dd>If set to nil, the layer will not learn an additive bias. default:t</dd></dl>
<h2 id="shape">Shape</h2><p>
<b>LinearLayer: (batch-size in-features) -&gt; (batch-size out-features)</b></p><dl><dt>Input</dt><dd>x (Tensor) where the x is the shape of (batch-size in-features)</dd><dt>Output</dt><dd>Output: an tensor that applied linearlayer, where the tensor is the shape of (batch-size out-features)</dd></dl><p>
</p><h2 id="37-forward">Forward</h2><p>
</p><pre><code class="lisp">(call (LinearLayer 10 1) x)
</code></pre><dl><dt>x</dt><dd>the input tensor</dd></dl><p>
</p><h2 id="38-example">Example</h2>
<pre><code class="lisp">(call (LinearLayer 10 1)(!randn `(10 10)))
</code></pre>
<p>
</p><h1 id="denselayer">DenseLayer</h1><p>
Calling LinearLayer, and activation specified in <code class="codex-param">activation</code>.</p><h2 id="39-parameters">Parameters</h2>
<pre><code class="lisp">(DenseLayer in-features out-features &amp;optional (bias t)(activation :relu))
</code></pre>
<dl><dt>in-features (fixnum)</dt><dd>size of each input sample</dd><dt>out-features (fixnum)</dt><dd>size of each output sample</dd><dt>bias (boolean)</dt><dd>If set to nil, the layer will not learn an additive bias. default:t</dd><dt>activation (keyword or function)</dt><dd>activation are following: :relu :sigmoid :tanh, If set to function, that is called as an activation.</dd></dl>
<h2 id="40-shape">Shape</h2><b>DenseLayer: (batch-size in-features) -&gt; (batch-size out-features)</b><dl><dt>Input</dt><dd>x (Tensor) where the x is the shape of (batch-size in-features)</dd><dt>Output</dt><dd>Output: an tensor that applied denselayer, where the tensor is the shape of (batch-size out-features)</dd></dl><p>
</p><h2 id="41-forward">Forward</h2><p>
</p><pre><code class="lisp">(call (DenseLayer 10 1) x)
</code></pre><dl><dt>x</dt><dd>the input tensor</dd></dl><p>
</p><h2 id="42-example">Example</h2>
<pre><code class="lisp">(call (DenseLayer 10 1)(!randn `(10 10)))
</code></pre>
<p>
</p><p>
</p><h1 id="dropout">Dropout</h1><p>
When *no-grad* is nil, dropout randomly zeroes some elements of the given tensor with sampling bernoulli tensor of <code class="codex-param">dropout-rate</code>.</p><p>Futhermore, the outputs are scaled by (/ (- 1 (self dropout-rate))), (i.e.: This is a Inverted Dropout.). This means when *no-grad* is t (i.e.: during predicting) dropout simply returns the given tensor.</p><h2 id="43-parameters">Parameters</h2><p>
</p><pre><code class="lisp">(dropout &amp;optional (dropout-rate 0.5))
</code></pre><dl><dt>dropout-rate</dt><dd>Dropout samples bernoulli distribution based on dropout-rate.</dd></dl><p>
</p><h2 id="44-shape">Shape</h2><b>Dropout: (Any) -&gt; (The same as a input)</b><dl><dt>Input</dt><dd>Any is OK</dd><dt>Output</dt><dd>The same as given input's shape.</dd></dl><p>
</p><h2 id="45-forward">Forward</h2>
<pre><code class="lisp">(setq x (!randn `(10 10)))
;#Const(((-0.59... -0.09... ~ 0.289... 0.390...)        
;                 ...
;        (1.447... 1.032... ~ -0.66... -0.55...)) :mgl t :shape (10 10))
(call (Dropout 0.5) x)
;#Const(((0.0 -0.19... ~ 0.0 0.0)        
;                 ...
;        (2.895... 2.064... ~ 0.0 -1.10...)) :mgl t :shape (10 10))
</code></pre>
<p>
</p><h1 id="batchnorm2d">BatchNorm2d</h1><p>Applies BatchNorm2D.</p><h2 id="46-parameters">Parameters</h2><p>
</p><pre><code class="lisp">(BatchNorm2D in-features &amp;key (affine t)(epsilon 1.0e-7))
</code></pre><dl><dt>in-features</dt><dd>an excepted input of size</dd><dt>affine</dt><dd>if t, the model has trainable affine layers.</dd><dt>epsilon</dt><dd>the value used to the denominator for numerical stability. Default: 1.0e-7</dd></dl><p>
</p><h2 id="47-shape">Shape</h2><p>
</p><pre><code class="lisp">(call (BatchNorm2D) x)
</code></pre><b>BatchNorm2D : (any, in-feature) -&gt; (the same as input of shape)</b><h2 id="48-example">Example</h2><p>
</p><pre><code class="lisp">(setq model (BatchNorm2D 10))
(call model (!randn `(30 10)))
</code></pre><p>
</p><h1 id="layernorm">LayerNorm</h1>


<h1 id="embedding">Embedding</h1><p>
A simple lookup table object to store embedding vectors for NLP Models.</p><h2 id="parameter">Parameter</h2>
<pre><code class="lisp">(Embedding vocab-size embedding-ize &amp;key pad-idx)
</code></pre>
<dl><dt>vocab-size</dt><dd>(fixnum) size of the dictionary of embeddings</dd><dt>embedding-size</dt><dd>(fixnum) the size of each embedding tensor</dd><dt>pad-idx</dt><dd>If specified, the entries at padding_idx do not contribute to the gradient. If nil, ignored.</dd></dl>
<h2 id="49-shape">Shape</h2><p>
</p><pre><code class="lisp">(call (Embedding 10 10) x)
</code></pre><p>
<b>Embedding: (batch-size sentence-length) -&gt; (batch-size sentence-len embedding-dim)</b></p><dl><dt>x</dt><dd>input x, where each element are single-float (like 1.0, 2.0 ...)</dd></dl><p>
</p><h2 id="50-example">Example</h2>
<pre><code class="lisp">(setq model (cl-waffe.nn:Embedding 10 20))

(call model (!ones `(1 10)))
#Const((((-0.01... -0.01... ~ 0.013... 0.002...)         
                   ...
         (-0.01... -0.01... ~ 0.013... 0.002...))) :mgl t :shape (1 10 20))
</code></pre>
<p>
</p><p>
</p><h1 id="rnn">RNN</h1><p>
Applies a multi-layer RNN with tanh or ReLU.</p><p>⚠️: RNN has performance problems when backward.</p><p>The assumption is that (setf !aref)'s backward contributes to it.</p><h2 id="51-parameters">Parameters</h2><p>
</p><pre><code class="lisp">(RNN input-size hidden-size &amp;key (num-layers 1)(activation :tanh)(bias t)(dropout nil)(biredical nil))
</code></pre><dl><dt>input-size</dt><dd>The number of excepted features of x</dd><dt>hidden-size</dt><dd>The number of features in hidden-layer</dd><dt>num-layers</dt><dd>Number of reccurent layers</dd><dt>activation</dt><dd>Can be either :tanh or :relu</dd><dt>bias</dt><dd>(boolean) If t, the model has a trainable bias.</dd><dt>dropout</dt><dd>(boolean) If t, the model has a dropout layer.</dd><dt>biredical</dt><dd>(boolean) If t, the model become a biredical RNN</dd></dl><p>
</p><h2 id="52-shape">Shape</h2><p>
</p><pre><code class="lisp">(call (RNN 10 10) x &amp;optional (hs nil))
</code></pre><p>
<b>RNN : (batch-size sentence-length input-size) -&gt; (batch-size sentence-length hidden-size)</b></p><dl><dt>x</dt><dd>the input x where the shape is (batch-size sentence-length input-size)</dd><dt>hs</dt><dd>The last hidden-state. if nil, the model creates a new one.</dd></dl><p>
</p><h2 id="53-example">Example</h2>
<pre><code class="lisp">(setq model (RNN 10 20))
(setq embedding (Embedding 10 10))
(call model
  (call embedding (!one `(10 10))))

;#Const((((-1.46... -1.46... ~ -5.53... 1.766...)         
;                   ...
;         (-1.46... -1.46... ~ -5.53... 1.766...))        
;                 ...
;        ((-1.46... -1.46... ~ -5.53... 1.766...)         
;                   ...
;         (-1.46... -1.46... ~ -5.53... 1.766...))) :mgl t :shape (10 10 20))
  
</code></pre>
<p>
</p><h1 id="lstm">LSTM</h1>


<h1 id="gru">GRU</h1>


<h1 id="maxpooling">MaxPooling</h1>


<h1 id="avgpooling">AvgPooling</h1>


<h1 id="conv1d">Conv1D</h1>


<h1 id="conv2d">Conv2D</h1>


<h1 id="transformer">Transformer</h1>


<h1 id="transformerencoderlayer">TransformerEncoderLayer</h1>


<h1 id="transformerdecoderlayer">TransformerDecoderLayer</h1>


<h1 id="crossentropy">CrossEntropy</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">cross-entropy</code><code class="codex-lambda-list">(x y &amp;optional (delta 1.0e-7) (epsilon 0.0))</code><div class="codex-docstring"><p>This criterion computes the cross entropy loss between x and y.</p><p>If epsilon is greater than 0.0, smooth-labeling is enabled.</p><p>If avoid-overrflow is t, x is substracted by x's average in order to avoid overflowing.</p><p>delta is the value used for (!log (!add x delta)).</p><p>x is a probability distribution.</p><p>y is a proablitity distribution or labels.</p><p>If y is labels, y is fixed to a probability distribution.</p></div></div>

<h1 id="softmaxcrossentropy">SoftMaxCrossEntropy</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">softmax-cross-entropy</code><code class="codex-lambda-list">(x y &amp;key (avoid-overflow t) (delta 1.0e-7) (epsilon 0.0))</code><div class="codex-docstring"><p>This criterion computes the softmax cross entropy loss between x and y.</p><p>If epsilon is greater than 0.0, smooth-labeling is enabled.</p><p>If avoid-overrflow is t, x is substracted by x's average in order to avoid overflowing.</p><p>delta is the value used for (!log (!add x delta)).</p><p>x is a probability distribution.</p><p>y is a proablitity distribution or labels.</p><p>If y is labels, y is fixed to a probability distribution.</p></div></div>

<h1 id="mse">MSE</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">mse</code><code class="codex-lambda-list">(p y)</code><div class="codex-docstring"><p>Computes MSE Loss.</p><p>mse is defined as (!mean (!pow (!sub p y) 2) 1)</p></div></div>

<h1 id="l1norm">L1Norm</h1>

<h1 id="l2norm">L2Norm</h1>

<h1 id="binarycrossentropy">BinaryCrossEntropy</h1>


<h1 id="kldivloss">KLdivLoss</h1>


<h1 id="cosinesimilarity">CosineSimilarity</h1>



      </div>
    </main>
  </article>
  <footer>
    <div class="info">
      Created with <a href="https://github.com/CommonDoc/codex">Codex</a>.
    </div>
  </footer>
  <script>
   HighlightLisp.highlight_auto();
  </script>

  </body>
</html>
