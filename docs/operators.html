<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
  Operators &ndash; cl-waffe
</title>
    <link rel="stylesheet" href="static/style.css"/>
    
  <link rel="stylesheet" href="static/highlight.css"/>
  <script src="static/highlight.js"></script>
  <style>
   /* Highlight the current top-level TOC item, and hide the TOC of all other items */

   .toc a[data-node="operators"] {
       /*color: #AD3108;*/
   }

   .toc ol {
       display: none;
   }

   .toc li a[data-node="operators"] {
       font-weight: bold;
   }

   .toc li a[data-node="operators"] + ol {
       display: block;
   }

   .toc li a[data-node="operators"] + ol li {
       margin-left: 10px;
   }
  </style>

  </head>
  <body>
    
  <h1 class="doc-title">cl-waffe</h1>
  <article id="article" data-section="operators">
    <aside>
      <ol class="toc"><li><a href="overview.html" data-node="overview">Overview</a><ol><li><a href="overview.html#welcome-to-cl-waffe!" data-node="welcome-to-cl-waffe!">Welcome to cl-waffe!</a></li><li><a href="overview.html#problems" data-node="problems">Problems</a></li><li><a href="overview.html#sections" data-node="sections">Sections</a></li><li><a href="overview.html#pull-requests" data-node="pull-requests">Pull Requests</a></li><li><a href="overview.html#contacts" data-node="contacts">Contacts</a></li><li><a href="overview.html#lla-setting" data-node="lla-setting">LLA Setting</a></li><li><a href="overview.html#when-memory-exhausted" data-node="when-memory-exhausted">When Memory Exhausted</a></li></ol></li><li><a href="mnist-tutorial.html" data-node="mnist-tutorial">MNIST Tutorial</a><ol><li><a href="mnist-tutorial.html#first" data-node="first">First</a></li><li><a href="mnist-tutorial.html#define-your-model" data-node="define-your-model">Define Your Model</a></li><li><a href="mnist-tutorial.html#define-your-dataset" data-node="define-your-dataset">Define Your Dataset</a><ol><li><a href="cl-waffe's-dataset--waffedataset.html" data-node="cl-waffe's-dataset--waffedataset">cl-waffe's Dataset: WaffeDataSet</a></li></ol></li><li><a href="mnist-tutorial.html#train-your-model" data-node="train-your-model">Train Your Model</a></li></ol></li><li><a href="extend-library.html" data-node="extend-library">Extend library</a><ol><li><a href="extend-library.html#exported" data-node="exported">Exported</a></li></ol></li><li><a href="using-tensor.html" data-node="using-tensor">Using Tensor</a><ol><li><a href="using-tensor.html#basic-tensor-operations" data-node="basic-tensor-operations">Basic Tensor Operations</a></li><li><a href="using-tensor.html#tensor" data-node="tensor">Tensor</a><ol><li><a href="using-tensor.html#basic-of-tensor-and-backward" data-node="basic-of-tensor-and-backward">Basic of Tensor and backward</a><ol><li><a href="using-tensor.html#initialize-tensor" data-node="initialize-tensor">Initialize Tensor</a><ol><li><a href="using-tensor.html#parameters" data-node="parameters">Parameters</a></li><li><a href="using-tensor.html#constants" data-node="constants">Constants</a></li><li><a href="using-tensor.html#tensor-vs-const" data-node="tensor-vs-const">Tensor vs Const</a></li></ol></li></ol></li><li><a href="using-tensor.html#forward-nodes" data-node="forward-nodes">Forward Nodes</a></li><li><a href="using-tensor.html#exported-parameters" data-node="exported-parameters">Exported Parameters</a></li><li><a href="using-tensor.html#types" data-node="types">Types</a></li><li><a href="using-tensor.html#accessor" data-node="accessor">Accessor</a></li></ol></li></ol></li><li><a href="cl-waffe.html" data-node="cl-waffe">cl-waffe</a><ol><li><a href="cl-waffe.html#package--cl-waffe" data-node="package--cl-waffe">Package :cl-waffe</a></li><li><a href="cl-waffe.html#0-sections" data-node="0-sections">Sections</a></li><li><a href="cl-waffe.html#model-and-node" data-node="model-and-node">Model And Node</a></li><li><a href="cl-waffe.html#defmodel" data-node="defmodel">defmodel</a></li><li><a href="cl-waffe.html#defnode" data-node="defnode">defnode</a></li><li><a href="cl-waffe.html#trainer" data-node="trainer">Trainer</a></li><li><a href="cl-waffe.html#deftrainer" data-node="deftrainer">deftrainer</a></li><li><a href="cl-waffe.html#datasets" data-node="datasets">Datasets</a></li><li><a href="cl-waffe.html#defdataset" data-node="defdataset">defdataset</a></li><li><a href="cl-waffe.html#optimizer" data-node="optimizer">optimizer</a></li><li><a href="cl-waffe.html#defoptimizer" data-node="defoptimizer">defoptimizer</a></li><li><a href="cl-waffe.html#documentation-template" data-node="documentation-template">Documentation Template</a></li></ol></li><li><a href="cl-waffe.nn.html" data-node="cl-waffe.nn">cl-waffe.nn</a><ol><li><a href="cl-waffe.nn.html#1-exported" data-node="1-exported">Exported</a></li></ol></li><li><a href="cl-waffe.optimizers.html" data-node="cl-waffe.optimizers">cl-waffe.optimizers</a><ol><li><a href="cl-waffe.optimizers.html#2-sections" data-node="2-sections">Sections</a></li></ol></li><li><a href="cl-waffe.io.html" data-node="cl-waffe.io">cl-waffe.io</a><ol><li><a href="cl-waffe.io.html#3-exported" data-node="3-exported">Exported</a></li></ol></li><li><a href="cl-waffe.caches.html" data-node="cl-waffe.caches">cl-waffe.caches</a><ol><li><a href="cl-waffe.caches.html#4-exported" data-node="4-exported">Exported</a></li></ol></li><li><a href="operators.html" data-node="operators">Operators</a><ol><li><a href="operators.html#!shape" data-node="!shape">!shape</a></li><li><a href="operators.html#!dims" data-node="!dims">!dims</a></li><li><a href="operators.html#!size" data-node="!size">!size</a></li><li><a href="operators.html#!zeros" data-node="!zeros">!zeros</a></li><li><a href="operators.html#!ones" data-node="!ones">!ones</a></li><li><a href="operators.html#!fill" data-node="!fill">!fill</a></li><li><a href="operators.html#!arange" data-node="!arange">!arange</a><ol><li><a href="(-!arange-stop-).html" data-node="(-!arange-stop-)">(!arange stop)</a></li><li><a href="(-!arange-start-stop-).html" data-node="(-!arange-start-stop-)">(!arange start stop)</a></li><li><a href="(-!arange-start-stop-step-).html" data-node="(-!arange-start-stop-step-)">(!arange start stop step)</a></li></ol></li><li><a href="operators.html#!random" data-node="!random">!random</a><ol><li><a href="when-limit=fixnum.html" data-node="when-limit=fixnum">When limit=fixnum</a></li><li><a href="when-limit=single-float.html" data-node="when-limit=single-float">When limit=single-float</a></li><li><a href="when-limit=-(-cons-single-float1-single-float2-).html" data-node="when-limit=-(-cons-single-float1-single-float2-)">When limit=(cons single-float1 single-float2)</a></li></ol></li><li><a href="operators.html#!random-with" data-node="!random-with">!random-with</a></li><li><a href="operators.html#!init-with" data-node="!init-with">!init-with</a></li><li><a href="operators.html#!normal" data-node="!normal">!normal</a></li><li><a href="operators.html#!randn" data-node="!randn">!randn</a></li><li><a href="operators.html#!beta" data-node="!beta">!beta</a></li><li><a href="operators.html#!gamma" data-node="!gamma">!gamma</a></li><li><a href="operators.html#!chisquare" data-node="!chisquare">!chisquare</a></li><li><a href="operators.html#!bernoulli" data-node="!bernoulli">!bernoulli</a></li><li><a href="operators.html#!binomial" data-node="!binomial">!binomial</a></li><li><a href="operators.html#5-!shape" data-node="5-!shape">!shape</a></li><li><a href="operators.html#6-!dims" data-node="6-!dims">!dims</a></li><li><a href="operators.html#7-!size" data-node="7-!size">!size</a></li><li><a href="operators.html#!zeros-like" data-node="!zeros-like">!zeros-like</a></li><li><a href="operators.html#!ones-like" data-node="!ones-like">!ones-like</a></li><li><a href="operators.html#!full-like" data-node="!full-like">!full-like</a></li><li><a href="operators.html#!add" data-node="!add">!add</a><ol><li><a href="examples.html" data-node="examples">Examples</a></li></ol></li><li><a href="operators.html#!sub" data-node="!sub">!sub</a><ol><li><a href="8-examples.html" data-node="8-examples">Examples</a></li></ol></li><li><a href="operators.html#!mul" data-node="!mul">!mul</a><ol><li><a href="9-examples.html" data-node="9-examples">Examples</a></li></ol></li><li><a href="operators.html#!div" data-node="!div">!div</a><ol><li><a href="10-examples.html" data-node="10-examples">Examples</a></li></ol></li><li><a href="operators.html#!dot" data-node="!dot">!dot</a><ol><li><a href="example.html" data-node="example">Example</a></li></ol></li><li><a href="operators.html#!sum" data-node="!sum">!sum</a><ol><li><a href="arguments.html" data-node="arguments">arguments</a></li><li><a href="11-example.html" data-node="11-example">Example</a></li></ol></li><li><a href="operators.html#!mean" data-node="!mean">!mean</a><ol><li><a href="12-example.html" data-node="12-example">Example</a></li></ol></li><li><a href="operators.html#!exp" data-node="!exp">!exp</a><ol><li><a href="13-example.html" data-node="13-example">Example</a></li></ol></li><li><a href="operators.html#!pow" data-node="!pow">!pow</a><ol><li><a href="14-example.html" data-node="14-example">Example</a></li></ol></li><li><a href="operators.html#!sqrt" data-node="!sqrt">!sqrt</a><ol><li><a href="15-example.html" data-node="15-example">Example</a></li></ol></li><li><a href="operators.html#!log" data-node="!log">!log</a><ol><li><a href="16-example.html" data-node="16-example">Example</a></li></ol></li><li><a href="operators.html#!sin" data-node="!sin">!sin</a><ol><li><a href="17-example.html" data-node="17-example">Example</a></li></ol></li><li><a href="operators.html#!cos" data-node="!cos">!cos</a><ol><li><a href="18-example.html" data-node="18-example">Example</a></li></ol></li><li><a href="operators.html#!tan" data-node="!tan">!tan</a><ol><li><a href="19-example.html" data-node="19-example">Example</a></li></ol></li><li><a href="operators.html#!asin" data-node="!asin">!asin</a></li><li><a href="operators.html#!acos" data-node="!acos">!acos</a></li><li><a href="operators.html#!atan" data-node="!atan">!atan</a></li><li><a href="operators.html#!sinh" data-node="!sinh">!sinh</a><ol><li><a href="20-example.html" data-node="20-example">Example</a></li></ol></li><li><a href="operators.html#!cosh" data-node="!cosh">!cosh</a><ol><li><a href="21-example.html" data-node="21-example">Example</a></li></ol></li><li><a href="operators.html#!tanh" data-node="!tanh">!tanh</a></li><li><a href="operators.html#!asinh" data-node="!asinh">!asinh</a></li><li><a href="operators.html#!acosh" data-node="!acosh">!acosh</a></li><li><a href="operators.html#!atanh" data-node="!atanh">!atanh</a></li><li><a href="operators.html#!matmul" data-node="!matmul">!matmul</a></li><li><a href="operators.html#!unsqueeze" data-node="!unsqueeze">!unsqueeze</a><ol><li><a href="22-example.html" data-node="22-example">Example</a></li></ol></li><li><a href="operators.html#!squeeze" data-node="!squeeze">!squeeze</a><ol><li><a href="23-example.html" data-node="23-example">Example</a></li></ol></li><li><a href="operators.html#!transpose" data-node="!transpose">!transpose</a><ol><li><a href="24-example.html" data-node="24-example">Example</a></li></ol></li><li><a href="operators.html#!transpose1" data-node="!transpose1">!transpose1</a><ol><li><a href="25-example.html" data-node="25-example">Example</a></li></ol></li><li><a href="operators.html#!repeats" data-node="!repeats">!repeats</a><ol><li><a href="26-example.html" data-node="26-example">Example</a></li></ol></li><li><a href="operators.html#!reshape" data-node="!reshape">!reshape</a><ol><li><a href="27-example.html" data-node="27-example">Example</a></li></ol></li><li><a href="operators.html#!abs" data-node="!abs">!abs</a></li><li><a href="operators.html#!where" data-node="!where">!where</a><ol><li><a href="28-example.html" data-node="28-example">Example</a></li></ol></li><li><a href="operators.html#!index" data-node="!index">!index</a></li><li><a href="operators.html#!filter" data-node="!filter">!filter</a></li><li><a href="operators.html#!argmax" data-node="!argmax">!argmax</a><ol><li><a href="29-example.html" data-node="29-example">Example</a></li></ol></li><li><a href="operators.html#!argmin" data-node="!argmin">!argmin</a><ol><li><a href="30-example.html" data-node="30-example">Example</a></li></ol></li><li><a href="operators.html#!<=" data-node="!<=">!&lt;=</a></li><li><a href="operators.html#!>=" data-node="!>=">!&gt;=</a></li><li><a href="operators.html#!einsum" data-node="!einsum">!einsum</a></li><li><a href="operators.html#!ravel" data-node="!ravel">!ravel</a></li><li><a href="operators.html#!flatten" data-node="!flatten">!flatten</a></li><li><a href="operators.html#!aref" data-node="!aref">!aref</a></li><li><a href="operators.html#!dotensors" data-node="!dotensors">!dotensors</a></li><li><a href="operators.html#!set-batch" data-node="!set-batch">!set-batch</a></li><li><a href="operators.html#!softmax" data-node="!softmax">!softmax</a></li><li><a href="operators.html#!sigmoid" data-node="!sigmoid">!sigmoid</a></li><li><a href="operators.html#!relu" data-node="!relu">!relu</a></li><li><a href="operators.html#!gelu" data-node="!gelu">!gelu</a></li><li><a href="operators.html#!leakey-relu" data-node="!leakey-relu">!leakey-relu</a></li><li><a href="operators.html#!swish" data-node="!swish">!swish</a></li></ol></li><li><a href="neural-networks.html" data-node="neural-networks">Neural Networks</a><ol><li><a href="model-list.html" data-node="model-list">model-list</a><ol><li><a href="31-parameters.html" data-node="31-parameters">Parameters</a></li><li><a href="forward.html" data-node="forward">Forward</a></li><li><a href="32-example.html" data-node="32-example">Example</a></li></ol></li><li><a href="linearlayer.html" data-node="linearlayer">Linearlayer</a><ol><li><a href="33-parameters.html" data-node="33-parameters">Parameters</a></li><li><a href="shape.html" data-node="shape">Shape</a></li><li><a href="34-forward.html" data-node="34-forward">Forward</a></li><li><a href="35-example.html" data-node="35-example">Example</a></li></ol></li><li><a href="denselayer.html" data-node="denselayer">DenseLayer</a><ol><li><a href="36-parameters.html" data-node="36-parameters">Parameters</a></li><li><a href="37-shape.html" data-node="37-shape">Shape</a></li><li><a href="38-forward.html" data-node="38-forward">Forward</a></li><li><a href="39-example.html" data-node="39-example">Example</a></li></ol></li><li><a href="dropout.html" data-node="dropout">Dropout</a><ol><li><a href="cl-waffe's-node--dropout.html" data-node="cl-waffe's-node--dropout">cl-waffe's Node: Dropout</a></li></ol></li><li><a href="batchnorm2d.html" data-node="batchnorm2d">BatchNorm2d</a><ol><li><a href="cl-waffe's-model--batchnorm2d.html" data-node="cl-waffe's-model--batchnorm2d">cl-waffe's Model: BatchNorm2d</a></li></ol></li><li><a href="layernorm.html" data-node="layernorm">LayerNorm</a></li><li><a href="embedding.html" data-node="embedding">Embedding</a><ol><li><a href="cl-waffe's-model--embedding.html" data-node="cl-waffe's-model--embedding">cl-waffe's Model: Embedding</a></li></ol></li><li><a href="rnn.html" data-node="rnn">RNN</a><ol><li><a href="cl-waffe's-model--rnn.html" data-node="cl-waffe's-model--rnn">cl-waffe's Model: RNN</a></li></ol></li><li><a href="lstm.html" data-node="lstm">LSTM</a></li><li><a href="gru.html" data-node="gru">GRU</a></li><li><a href="maxpooling.html" data-node="maxpooling">MaxPooling</a></li><li><a href="avgpooling.html" data-node="avgpooling">AvgPooling</a></li><li><a href="conv1d.html" data-node="conv1d">Conv1D</a></li><li><a href="conv2d.html" data-node="conv2d">Conv2D</a></li><li><a href="transformer.html" data-node="transformer">Transformer</a></li><li><a href="transformerencoderlayer.html" data-node="transformerencoderlayer">TransformerEncoderLayer</a></li><li><a href="transformerdecoderlayer.html" data-node="transformerdecoderlayer">TransformerDecoderLayer</a></li><li><a href="crossentropy.html" data-node="crossentropy">CrossEntropy</a></li><li><a href="softmaxcrossentropy.html" data-node="softmaxcrossentropy">SoftMaxCrossEntropy</a></li><li><a href="mse.html" data-node="mse">MSE</a></li><li><a href="l1norm.html" data-node="l1norm">L1Norm</a></li><li><a href="l2norm.html" data-node="l2norm">L2Norm</a></li><li><a href="binarycrossentropy.html" data-node="binarycrossentropy">BinaryCrossEntropy</a></li><li><a href="kldivloss.html" data-node="kldivloss">KLdivLoss</a></li><li><a href="cosinesimilarity.html" data-node="cosinesimilarity">CosineSimilarity</a></li></ol></li><li><a href="optimizers.html" data-node="optimizers">Optimizers</a><ol><li><a href="sgd.html" data-node="sgd">SGD</a><ol><li><a href="cl-waffe's-optimizer--sgd.html" data-node="cl-waffe's-optimizer--sgd">cl-waffe's Optimizer: SGD</a></li></ol></li><li><a href="momentum.html" data-node="momentum">Momentum</a><ol><li><a href="cl-waffe's-optimizer--momentum.html" data-node="cl-waffe's-optimizer--momentum">cl-waffe's Optimizer: Momentum</a></li></ol></li><li><a href="adagrad.html" data-node="adagrad">AdaGrad</a><ol><li><a href="cl-waffe's-optimizer--adagrad.html" data-node="cl-waffe's-optimizer--adagrad">cl-waffe's Optimizer: AdaGrad</a></li></ol></li><li><a href="rmsprop.html" data-node="rmsprop">RMSProp</a><ol><li><a href="cl-waffe's-optimizer--rmsprop.html" data-node="cl-waffe's-optimizer--rmsprop">cl-waffe's Optimizer: RMSProp</a></li></ol></li><li><a href="adam.html" data-node="adam">Adam</a><ol><li><a href="cl-waffe's-optimizer--adam.html" data-node="cl-waffe's-optimizer--adam">cl-waffe's Optimizer: Adam</a></li></ol></li><li><a href="adamw.html" data-node="adamw">AdamW</a></li><li><a href="radam.html" data-node="radam">RAdam</a></li></ol></li></ol>
    </aside>
    <main class="codex-section">
      <header>
        <h2 class="section-title">Operators</h2>
      </header>
      <div class="content">
        <h1 id="!shape">!shape</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!shape</code><code class="codex-lambda-list">(tensor &amp;optional (nth nil))</code><div class="codex-docstring"><p>Returns the shape of tensor when nth=nil.<code class="codex-param">nth</code> indicates the index of shape, !shape return specified value.</p><p>Example:
</p><pre><code class="lisp">(setq a (!randn `(10 10 10)))
(!shape a) ; =&gt; (10 10 10)
(!shape a 0) ;=&gt; 10
</code></pre></div></div>

<h1 id="!dims">!dims</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!dims</code><code class="codex-lambda-list">(tensor)</code><div class="codex-docstring"><p>Returns the total length of a given tensor's dims</p><p>Example:
</p><pre><code class="lisp">(!dims (!zeros '(10 10 10))) ; =&gt; 3
</code></pre></div></div>

<h1 id="!size">!size</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!size</code><code class="codex-lambda-list">(tensor)</code><div class="codex-docstring"><p>Returns the total size of a tensor</p><p>Example:
</p><pre><code class="lisp">(!size (!zeros '(10 10 10))) ; =&gt; 1000
</code></pre></div></div>

<h1 id="!zeros">!zeros</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!zeros</code><code class="codex-lambda-list">(shape)</code><div class="codex-docstring"><p>Initializing constant tensor with given shape, where initial elements are zero.</p><p>Input: shape (cons)</p><p>Output: Tensor (which is constant)</p><p>Example:
</p><pre><code class="lisp">(!zeros `(10 10))
;#Const(((0.0 0.0 ~ 0.0 0.0)        
;                ...
;        (0.0 0.0 ~ 0.0 0.0)) :mgl t :shape (10 10))
</code></pre></div></div>

<h1 id="!ones">!ones</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!ones</code><code class="codex-lambda-list">(shape)</code><div class="codex-docstring"><p>The same as !zeros but initial element is one.</p><p>Example:
</p><pre><code class="lisp">(!ones `(10 10))
;#Const(((1.0 1.0 ~ 1.0 1.0)        
;                ...
;        (1.0 1.0 ~ 1.0 1.0)) :mgl t :shape (10 10))
</code></pre></div></div>

<p>
</p><h1 id="!fill">!fill</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!fill</code><code class="codex-lambda-list">(shape element)</code><div class="codex-docstring"><p>The same as !zeros, !ones but initial element is given element.</p><p>Note: the argument <code class="codex-param">element</code> coerced into <code class="codex-param">mgl-mat:*default-mat-ctype*</code></p><p>Example:
</p><pre><code class="lisp">(!fill '(10 10) 10)
;#Const(((10.0 10.0 ~ 10.0 10.0)        
;                  ...
;        (10.0 10.0 ~ 10.0 10.0)) :mgl t :shape (10 10))
</code></pre><p>
</p></div></div>

<p>
</p><h1 id="!arange">!arange</h1>

<div class="codex-doc-node codex-operator codex-macro"><code class="codex-name">!arange</code><code class="codex-lambda-list">(&amp;rest args)</code><div class="codex-docstring"><p>Like numpy's arange, arange can be called with a varying number of positional arguments:</p><h2 id="(-!arange-stop-)">(!arange stop)</h2>
<pre><code class="lisp">(!arange 10)
;#Const((0.0 1.0 ~ 8.0 9.0) :mgl t :shape (10))
</code></pre>
<h2 id="(-!arange-start-stop-)">(!arange start stop)</h2>
<pre><code class="lisp">(!arange 3 10)
;=&gt;#Const((3.0 4.0 ~ 8.0 9.0) :mgl t :shape (7))
</code></pre>
<h2 id="(-!arange-start-stop-step-)">(!arange start stop step)</h2>
<pre><code class="lisp">(!arange 3 10 2)
;#Const((3.0 5.0 7.0 9.0) :mgl t :shape (4))
</code></pre>
</div></div>

<h1 id="!random">!random</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!random</code><code class="codex-lambda-list">(dims limit)</code><div class="codex-docstring"><p>Initialize an tensor of dims (cons)</p><p>!random can be called with a varying number of type of arguments:</p><h2 id="when-limit=fixnum">When limit=fixnum</h2><p>
init within the range of <code>0&lt;=x&lt;limit</code></p><pre><code class="lisp">;#Const(((1.0 2.0 ~ 2.0 1.0)        
;                 ...
;        (2.0 2.0 ~ 2.0 2.0)) :mgl t :shape (10 10))
</code></pre><p>
</p><h2 id="when-limit=single-float">When limit=single-float</h2>
init within the range of <code>0&lt;=x&lt;limit</code>
<pre><code class="lisp">(!random '(10 10) 3.0)
;#Const(((0.152... 2.203... ~ 2.360... 2.216...)        
;                 ...
;        (1.003... 2.257... ~ 2.305... 2.025...)) :mgl t :shape (10 10))
</code></pre>
<h2 id="when-limit=-(-cons-single-float1-single-float2-)">When limit=(cons single-float1 single-float2)</h2>
init with single-float1&lt;=x&lt;single-float2, where each element is single-float.
<pre><code class="lisp">(!random '(10 10) '(1.0 3.0))
;#Const(((1.982... 1.526... ~ 1.388... 1.312...)        
;                 ...
;        (1.829... 2.676... ~ 1.226... 2.980...)) :mgl t :shape (10 10))
</code></pre>
<p>Return: WaffeTensor
</p></div></div>

<p>
</p><h1 id="!random-with">!random-with</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!random-with</code><code class="codex-lambda-list">(dims f)</code><div class="codex-docstring"><p>Initializes the tensor of dims. Each element is initialized with <code class="codex-param">f</code> where f is a lambda exp and called with index.</p><p>Warning: Using mref and slow algorithm, <b>it is so slow</b>.</p><p>Example:
</p><pre><code class="lisp">(!random-with '(10 10) #'(lambda (n) n))
;#Const(((0.0 1.0 ~ 8.0 9.0)        
;                 ...
;        (90.0 91.0 ~ 98.0 99.0)) :mgl t :shape (10 10))
</code></pre><p>See also: !init-with which is alias for !random-with.
</p></div></div>

<h1 id="!init-with">!init-with</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!init-with</code><code class="codex-lambda-list">(dims f)</code><div class="codex-docstring">Alias for !random-with. This function is inlined.</div></div>

<p>
</p><h1 id="!normal">!normal</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!normal</code><code class="codex-lambda-list">(dims &amp;optional (mean 2.0) (var 1.0))</code><div class="codex-docstring"><p>Init with normal distribution.</p><p>Warning: Using mref and slow algorithm, <b>its sooo slow.</b></p><p>It is recommended to use !randn and transform it instead.</p></div></div>

<p>
</p><h1 id="!randn">!randn</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!randn</code><code class="codex-lambda-list">(dims)</code><div class="codex-docstring"><p>Initializes tensor with normal distribution in a faster way where mean=0.0, var=1.0.</p><p>Example:</p><pre><code class="lisp">(!randn `(10 10))
;#Const(((0.677... 0.054... ~ 0.257... 0.261...)        
;                 ...
;        (0.063... 0.607... ~ 0.460... 0.730...)) :mgl t :shape (10 10))
</code></pre></div></div>

<p>
</p><h1 id="!beta">!beta</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!beta</code><code class="codex-lambda-list">(dims alpha beta)</code><div class="codex-docstring"><p>Initializes tensor with samples of beta distribution in a faster way.</p><p>Algorithm: https://dl.acm.org/doi/pdf/10.1145/359460.359482</p><p>x=[0,1]</p><p>a = min(alpha, beta)</p><p>b = max(alpha, beta)</p><p>PDF: fX(x)=x^a−1*(1−x)*b−1/B(a,b)</p><p>where B(a,b)=∫1,0{x^a−1(1−x)^b−1}dx</p><pre><code class="lisp">(time (!beta '(200) 5.0 1.0))
;Evaluation took:
;  0.000 seconds of real time
;  0.000063 seconds of total run time (0.000063 user, 0.000000 system)
;  100.00% CPU
;  143,846 processor cycles
;  0 bytes consed
  
;#Const((0.813... 0.832... ~ 0.865... 0.787...) :mgl t :shape (200))
</code></pre></div></div>

<p>
</p><h1 id="!gamma">!gamma</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!gamma</code><code class="codex-lambda-list">(dims k &amp;optional (theta 1.0))</code><div class="codex-docstring"><p>Initialize tensor with samples of gamma distribution.</p><p>Todo: Use fast algorithms and approximations in response to <code class="codex-param">k</code>.</p><p>Example:
</p><pre><code class="lisp">(!gamma '(10 10) 1.0)
;#Const(((2.155... 3.374... ~ 1.274... 0.147...)        
;                 ...
;        (0.194... 0.081... ~ 0.816... 0.209...)) :mgl t :shape (10 10))
</code></pre></div></div>

<p>
</p><h1 id="!chisquare">!chisquare</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!chisquare</code><code class="codex-lambda-list">(dims df)</code><div class="codex-docstring"><p><b>Not implemented yet</b>
Todo: Use fast algorithms and approximations.</p><p>Example:
</p><pre><code class="lisp"></code></pre></div></div>

<p>
</p><h1 id="!bernoulli">!bernoulli</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!bernoulli</code><code class="codex-lambda-list">(dims rate)</code><div class="codex-docstring"><p>Init a tensor of dims with bernoulli</p><p>rate is single-float, and [0 1]</p><p>See also: <code class="codex-param">!binomial</code>, alias for it.</p><p>Example:
</p><pre><code class="lisp">(!binomial '(10 10) 0.5)
;#Const(((1.0 0.0 ~ 1.0 1.0)        
;                 ...
;        (0.0 1.0 ~ 1.0 0.0)) :mgl t :shape (10 10))
</code></pre></div></div>

<h1 id="!binomial">!binomial</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!binomial</code><code class="codex-lambda-list">(dims rate)</code><div class="codex-docstring">Alias for !bernoulli</div></div>

<p>
</p><h1 id="5-!shape">!shape</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!shape</code><code class="codex-lambda-list">(tensor &amp;optional (nth nil))</code><div class="codex-docstring"><p>Returns the shape of tensor when nth=nil.<code class="codex-param">nth</code> indicates the index of shape, !shape return specified value.</p><p>Example:
</p><pre><code class="lisp">(setq a (!randn `(10 10 10)))
(!shape a) ; =&gt; (10 10 10)
(!shape a 0) ;=&gt; 10
</code></pre></div></div>

<p>
</p><h1 id="6-!dims">!dims</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!dims</code><code class="codex-lambda-list">(tensor)</code><div class="codex-docstring"><p>Returns the total length of a given tensor's dims</p><p>Example:
</p><pre><code class="lisp">(!dims (!zeros '(10 10 10))) ; =&gt; 3
</code></pre></div></div>

<p>
</p><h1 id="7-!size">!size</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!size</code><code class="codex-lambda-list">(tensor)</code><div class="codex-docstring"><p>Returns the total size of a tensor</p><p>Example:
</p><pre><code class="lisp">(!size (!zeros '(10 10 10))) ; =&gt; 1000
</code></pre></div></div>

<p>
</p><h1 id="!zeros-like">!zeros-like</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!zeros-like</code><code class="codex-lambda-list">(tensor)</code><div class="codex-docstring"><p>Return a const where the shape is the same as tensor but elements are zero.</p><p>Example:
</p><pre><code class="lisp">(setq a (!randn `(10 10)))
(!zeros-like a)
;#Const(((0.0 0.0 ~ 0.0 0.0)        
;                 ...
;        (0.0 0.0 ~ 0.0 0.0)) :mgl t :shape (10 10))
</code></pre></div></div>

<p>
</p><h1 id="!ones-like">!ones-like</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!ones-like</code><code class="codex-lambda-list">(tensor)</code><div class="codex-docstring">Return a const where the shape is the same as tensor but elements are one.
Example:
<pre><code class="lisp">(setq a (!randn `(10 10)))
(!ones-like a)
;#Const(((1.0 1.0 ~ 1.0 1.0)        
;                 ...
;        (1.0 1.0 ~ 1.0 1.0)) :mgl t :shape (10 10))
</code></pre></div></div>

<p>
</p><h1 id="!full-like">!full-like</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!full-like</code><code class="codex-lambda-list">(tensor element)</code><div class="codex-docstring">Return a const where the shape is the same as tensor but elements are specified value by <code class="codex-param">element</code>.
Example:
<pre><code class="lisp">(setq a (!randn `(10 10)))
(!full-like a 3)
;#Const(((3.0 3.0 ~ 3.0 3.0)        
;                 ...
;        (3.0 3.0 ~ 3.0 3.0)) :mgl t :shape (10 10))
</code></pre></div></div>

<h1 id="!add">!add</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!add</code><code class="codex-lambda-list">(x y)</code><div class="codex-docstring"><p>Adds x and y.</p><p>In the case when x or y is not a tensor, automatically creates a new tensor.</p><p>It supports:</p><ol><li>Broadcasting shapes</li><li>JIT</li></ol><h2 id="examples">Examples</h2>
<pre><code class="lisp">(setq a (!randn `(3 3)))
(setq b (!randn `(3 3)))
(setq c (!randn `(3 1)))

(!add 1 1)
;=&gt; Const(2)

(!add (const 1)(const 1))
;=&gt; Const(2)

(!add a b)
;#Const(((3.418... 1.974... 0.177...)
;                 ...
;        (-1.30... 0.987... 1.917...)) :mgl t :shape (3 3))

(!add a c)
;#Const(((1.426... 2.129... 1.050...)
;                 ...
;        (-0.64... 0.269... 0.303...)) :mgl t :shape (3 3))

</code></pre>
</div></div>

<h1 id="!sub">!sub</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!sub</code><code class="codex-lambda-list">(x y)</code><div class="codex-docstring"><p>Subtract x by y.</p><p>In the case when x or y is not a tensor, automatically creates a new tensor.</p><p>It supports:</p><ol><li>Broadcasting shapes</li><li>JIT</li></ol><h2 id="8-examples">Examples</h2>
<pre><code class="lisp">(setq a (!randn `(3 3)))
(setq b (!randn `(3 3)))
(setq c (!randn `(3 1)))

(!sub 1 1)
;=&gt; Const(0)

(!sub (const 1)(const 1))
;=&gt; Const(0)

(!sub a b)
;#Const(((-0.86... 1.413... 1.139...)
;                 ...
;        (0.017... -0.44... -1.31...)) :mgl t :shape (3 3))

(!sub a c)
;#Const(((1.128... 1.258... 0.267...)
;                 ...
;        (-0.64... 0.269... 0.303...)) :mgl t :shape (3 3))

</code></pre>
<p>
</p></div></div>

<h1 id="!mul">!mul</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!mul</code><code class="codex-lambda-list">(x y)</code><div class="codex-docstring"><p>Multiply x and y with element-wise.</p><p>In the case when x or y is not a tensor, automatically creates a new tensor.</p><p>It supports:</p><ol><li>Broadcasting shapes</li><li>JIT</li></ol><h2 id="9-examples">Examples</h2>
<pre><code class="lisp">(setq a (!randn `(3 3)))
(setq b (!randn `(3 3)))
(setq c (!randn `(3 1)))

(!mul 1 1)
;=&gt; Const(1)

(!mul (const 1)(const 1))
;=&gt; Const(1)

(!mul a b)
;#Const(((2.734... 0.475... -0.31...)        
;                 ...
;        (0.426... 0.193... 0.490...)) :mgl t :shape (3 3))

(!mul a c)
;#Const(((2.734... 0.475... -0.31...)        
;                 ...
;        (0.426... 0.193... 0.490...)) :mgl t :shape (3 3))

</code></pre>
<p>
</p></div></div>

<h1 id="!div">!div</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!div</code><code class="codex-lambda-list">(x y)</code><div class="codex-docstring"><p>Divides x by y.</p><p>In the case when x or y is not a tensor, automatically creates a new tensor.</p><p>It supports:</p><ol><li>Broadcasting shapes</li><li>JIT</li></ol><h2 id="10-examples">Examples</h2>
<pre><code class="lisp">(setq a (!randn `(3 3)))
(setq b (!ones `(3 3)))
(setq c (!ones `(3 1)))

(!div 2 1)
;=&gt; Const(2)

(!div (const 2)(const 1))
;=&gt; Const(2)

(!div a b)
;#Const(((1.734... 0.475... -0.31...)        
;                 ...
;        (0.426... 0.193... 0.490...)) :mgl t :shape (3 3))

(!div a c)
;#Const(((2.734... 0.475... -0.31...)        
;                 ...
;        (0.426... 0.193... 0.490...)) :mgl t :shape (3 3))

</code></pre>
<p>
</p></div></div>

<h1 id="!dot">!dot</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!dot</code><code class="codex-lambda-list">(x y)</code><div class="codex-docstring"><p>Computes the dot product of x and y where x and y are 1d Tensor.</p><p>🗒Note: Unlike Numpy's dot, !dot only supports for 1d tensors with the same number of elements and the tensor of which dims is larger than 1, regarded as 1d tensors.</p><h2 id="example">Example</h2>
<pre><code class="lisp">(setq a (!randn `(10)))
(setq b (!randn `(10)))

(!dot a b)
;=&gt; #Const(1.0842022e-19)
</code></pre>
<p>
</p></div></div>

<h1 id="!sum">!sum</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!sum</code><code class="codex-lambda-list">(x &amp;optional (axis nil) (keepdims nil))</code><div class="codex-docstring"><p>Sum up x where x is a cl-waffe tensor.</p><p>For nd tensors...
</p><dl><dt>1D</dt><dd>unsqueeze x with 1, and call !sum again.</dd><dt>2D and more.</dt><dd>Sum up all elements of X</dd></dl><h2 id="arguments">arguments</h2><dl><dt>axis</dt><dd>a dimension to reduce</dd><dt>keepdims</dt><dd>When t, the returning tensor is repeated with <code class="codex-param">axis</code></dd></dl><h2 id="11-example">Example</h2>
<pre><code class="lisp">(setq a (!randn `(10)))
(!sum a)
;=&gt;#Const(4.74653)

(setq a (!randn `(10 10)))
(!sum a)
;=&gt;#Const(1.5428619)

(!sum a 0)
;=&gt;#Const(((-2.07... 0.463... ~ 1.778... 1.695...)) :mgl t :shape (1 10))

(!sum a 1)
;#Const(((0.967...)        
;                 ...
;        (2.774...)) :mgl t :shape (10 1))

(!sum a 0 t)
;#Const(((-2.07... 0.463... ~ 1.778... 1.695...)        
;                 ...
;        (-2.07... 0.463... ~ 1.778... 1.695...)) :mgl t :shape (10 10))
</code></pre>
<p>
</p></div></div>

<p>
</p><h1 id="!mean">!mean</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!mean</code><code class="codex-lambda-list">(x &amp;optional (axis nil) (keepdims nil))</code><div class="codex-docstring">The usage is the same as !sum.<h2 id="12-example">Example</h2>
<pre><code class="lisp">(setq a (!ones '(10 10)))
;#Const(((1.0 1.0 ~ 1.0 1.0)        
;                 ...
;        (1.0 1.0 ~ 1.0 1.0)) :mgl t :shape (10 10))
(!mean a)
;=&gt;Const(1.0)
</code></pre>
</div></div>

<h1 id="!exp">!exp</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!exp</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring">Applying exp to each element of x, creating a new sysconst.<h2 id="13-example">Example</h2>
<pre><code class="lisp">(setq a (!randn `(10 10)))
;#Const(((0.624... 0.807... ~ 0.500... 0.937...)        
;                 ...
;        (0.662... 0.299... ~ 0.761... 0.729...)) :mgl t :shape (10 10))
(!exp a)
;#Const(((1.866... 2.242... ~ 1.650... 2.553...)        
;                 ...
;        (1.939... 1.349... ~ 2.140... 2.073...)) :mgl t :shape (10 10))
</code></pre>
</div></div>

<h1 id="!pow">!pow</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!pow</code><code class="codex-lambda-list">(x n)</code><div class="codex-docstring">Takes the power of each element in <code class="codex-param">x</code> with n, returning a new sysconst.<h2 id="14-example">Example</h2>
<pre><code class="lisp">(setq a (!ones `(10 10)))
(!pow a 3)
;#Const(((1.0 1.0 ~ 1.0 1.0)        
;                 ...
;        (1.0 1.0 ~ 1.0 1.0)) :mgl t :shape (10 10))
</code></pre>
</div></div>

<p>
</p><h1 id="!sqrt">!sqrt</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!sqrt</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring">Takes the power of eachelement in <code class="codex-param">x</code> with 1/2, creating new sysconst and nodes.<h2 id="15-example">Example</h2>
<pre><code class="lisp">(setq a (!ones `(10 10)))
(!sqrt a 3)
;#Const(((1.0 1.0 ~ 1.0 1.0)
;                 ...
;        (1.0 1.0 ~ 1.0 1.0)) :mgl t :shape (10 10))
</code></pre>
</div></div>

<p>
</p><h1 id="!log">!log</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!log</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring"><p>Returns a new tensor with the natural logarithm of the elements of input.</p><p>yi = log(e xi)</p><h2 id="16-example">Example</h2>
<pre><code class="lisp">(setq a (!ones '(10 10)))
(!log a)
;#Const(((0.0 0.0 ~ 0.0 0.0)        
;                 ...
;        (0.0 0.0 ~ 0.0 0.0)) :mgl t :shape (10 10))
</code></pre>
</div></div>

<h1 id="!sin">!sin</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!sin</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring">Applying sin to each element of x, creating a new sysconst.<h2 id="17-example">Example</h2>
<pre><code class="lisp">(setq a (!randn `(5)))
;=&gt;#Const((0.638... 0.527... 0.515... 0.495... 0.912...) :mgl t :shape (5))
(!sin a)
;=&gt;#Const((-0.44... -0.64... -0.66... -0.70... -0.09...) :mgl t :shape (5))
</code></pre>
</div></div>

<h1 id="!cos">!cos</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!cos</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring">Applying cos to each element of x, creating a new sysconst.<h2 id="18-example">Example</h2>
<pre><code class="lisp">(setq a (!randn `(5)))
;=&gt;#Const((0.638... 0.527... 0.515... 0.495... 0.912...) :mgl t :shape (5))
(!cos a)
;=&gt;#Const((0.803... 0.864... 0.870... 0.879... 0.611...) :mgl t :shape (5))
</code></pre>
</div></div>

<h1 id="!tan">!tan</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!tan</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring">Applying tan to each element of x, creating a new sysconst.<h2 id="19-example">Example</h2>
<pre><code class="lisp">(setq a (!randn `(5)))
;=&gt;#Const((0.638... 0.527... 0.515... 0.495... 0.912...) :mgl t :shape (5))
(!tan a)
;=&gt;#Const((0.741... 0.582... 0.566... 0.540... 1.293...) :mgl t :shape (5))
</code></pre>
</div></div>

<h1 id="!asin">!asin</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!asin</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring">Applying asin to each element</div></div>

<h1 id="!acos">!acos</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!acos</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring">Applying acos to each element</div></div>

<h1 id="!atan">!atan</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!atan</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring">Applying atan to each element</div></div>

<h1 id="!sinh">!sinh</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!sinh</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring">Applying sinh to each element of x, creating a new sysconst.<h2 id="20-example">Example</h2>
<pre><code class="lisp">(setq a (!randn `(5)))
;=&gt;#Const((0.638... 0.527... 0.515... 0.495... 0.912...) :mgl t :shape (5))
(!sinh a)
;=&gt;#Const((0.682... 0.551... 0.538... 0.516... 1.044...) :mgl t :shape (5))
</code></pre>
</div></div>

<h1 id="!cosh">!cosh</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!cosh</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring">Applying cosh to each element of x, creating a new sysconst.<h2 id="21-example">Example</h2>
<pre><code class="lisp">(setq a (!randn `(5)))
;=&gt;#Const((0.638... 0.527... 0.515... 0.495... 0.912...) :mgl t :shape (5))
(!cosh a)
;=&gt;#Const((1.210... 1.142... 1.135... 1.125... 1.446...) :mgl t :shape (5))
</code></pre>
</div></div>

<h1 id="!tanh">!tanh</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!tanh</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring">Applying tanh to x, return a new sysconst with making nodes.</div></div>

<h1 id="!asinh">!asinh</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!asinh</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring">Applying asinh to each element</div></div>

<h1 id="!acosh">!acosh</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!acosh</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring">Applying acosh to each element</div></div>

<h1 id="!atanh">!atanh</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!atanh</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring">Applying atanh to each element</div></div>

<h1 id="!matmul">!matmul</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!matmul</code><code class="codex-lambda-list">(x y)</code><div class="codex-docstring"><p>Multiplying matrices <code class="codex-param">x</code> and <code class="codex-param">y</code>.</p><p>!matmul has many behaviours depends on the dimensionality of the tensors as follows:</p><dl><dt>x and y are 1D</dt><dd>The dot-product is returned.
<pre><code class="lisp">(setq a (!randn `(10)))
(setq b (!randn `(10)))
(!matmul a b)
;=&gt;#Const(-2.0)
</code></pre>
</dd><dt>x and y are both 2D</dt><dd>The matrix-matrix product is returned.
<pre><code class="lisp">(setq a (!randn `(3 10)))
(setq b (!randn `(10 3)))
(!matmul a b)
;#Const(((2.309... 2.223... 3.630...)        
;                 ...
;        (2.334... 2.850... 3.678...)) :mgl t :shape (3 3))
</code></pre>
</dd><dt>x is 2D and y is 3D.</dt><dd>The matrix and y's each matrix are multiplied and is returned.
<pre><code class="lisp">(setq a (!randn `(3 10)))
(setq b (!randn `(5 10 3)))

(!matmul a b)
;(!aref b 0) ~ (!aref b 4) is multiplied with a

;#Const((((3.257... 2.731... 1.670...)         
;                   ...
;         (2.523... 2.251... 1.276...))        
;                 ...
;        ((2.610... 2.764... 2.415...)         
;                   ...
;         (2.080... 2.204... 1.751...))) :mgl t :shape (5 3 3))
</code></pre>
</dd><dt>x is 3D and y is 2D.</dt><dd>The matrix and x's each matrix are multiplied and is returned.
<pre><code class="lisp">(setq a (!randn `(5 3 10)))
(setq b (!randn `(10 3)))

(!matmul a b)
;(!aref a 0) ~ (!aref a 4) is multiplied with b
;#Const((((2.309... 2.204... 1.556...)         
;                   ...
;         (3.746... 3.869... 3.091...))        
;                 ...
;        ((3.260... 3.200... 2.847...)         
;                   ...
;         (3.008... 2.186... 2.376...))) :mgl t :shape (5 3 3))
</code></pre>
</dd><dt>For more...</dt><dd>More will be added (e.g.: 1d and 2d, for larger than 4d ...)</dd></dl></div></div>

<h1 id="!unsqueeze">!unsqueeze</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!unsqueeze</code><code class="codex-lambda-list">(x &amp;optional (dim 0) (count 1))</code><div class="codex-docstring"><p>Returns a new tensor with a dimension of size one inserted at the specified position.</p><p>dim indicates the position, when dim=-1, it indicates a last dimension of <code class="codex-param">x</code>.</p><h2 id="22-example">Example</h2>
<pre><code class="lisp">(setq a (!randn `(10 10)))
;#Const(((0.685... 0.827... ~ 0.076... 0.102...)        
;                 ...
;        (0.802... 0.571... ~ 0.207... 0.283...)) :mgl t :shape (10 10))
(!unsqueeze a)
;#Const((((0.685... 0.827... ~ 0.076... 0.102...)         
;                   ...
;         (0.802... 0.571... ~ 0.207... 0.283...))) :mgl t :shape (1 10 10))

(!unsqueeze a -1)
;#Const((((0.685...)         
;                   ...
;         (0.102...))        
;                 ...
;        ((0.802...)         
;                   ...
;         (0.283...))) :mgl t :shape (10 10 1))

(!unsqueeze a 2)
;#Const(((0.685... 0.827... ~ 0.076... 0.102...)        
;                 ...
;        (0.802... 0.571... ~ 0.207... 0.283...)) :mgl t :shape (10 10 1 1))
</code></pre>
</div></div>

<h1 id="!squeeze">!squeeze</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!squeeze</code><code class="codex-lambda-list">(x &amp;optional (dim nil))</code><div class="codex-docstring"><p>Returns a new tensor with a dimension of size one removed at the specified position.</p><p>When dim=nil or -1, the last position of dim will be removed.</p><p>If the specified position of a tensor isn't one, !squeeze is skipped.</p><h2 id="23-example">Example</h2>
<pre><code class="lisp">(setq a (!randn `(10 1 10)))
;#Const((((0.928... 0.556... ~ 0.697... 0.973...))        
;                 ...
;        ((0.368... 0.995... ~ 0.589... 0.716...))) :mgl t :shape (10 1 10))

(!squeeze a 1)
;#Const(((0.928... 0.556... ~ 0.697... 0.973...)        
;                 ...
;        (0.368... 0.995... ~ 0.589... 0.716...)) :mgl t :shape (10 10))

(!squeeze a -1)
;#Const((((0.928... 0.556... ~ 0.697... 0.973...))        
;                 ...
;        ((0.368... 0.995... ~ 0.589... 0.716...))) :mgl t :shape (10 1 10))

(setq a (!randn `(10 10 1)))
;#Const(((0.991... 0.248... ~ 0.610... 0.289...)        
;                 ...
;        (0.593... 0.177... ~ 0.374... 0.668...)) :mgl t :shape (10 10))
</code></pre>
</div></div>

<h1 id="!transpose">!transpose</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!transpose</code><code class="codex-lambda-list">(x &amp;optional result)</code><div class="codex-docstring"><p>Transpose x where x is a 2d tensor.</p><p>Transposed x is lazy evaluated until called by !matmul.</p><p>Todo: implement 3d, 4d version...</p><h2 id="24-example">Example</h2>
<pre><code class="lisp">(setq a (!randn `(3 5)))
(setq a (!transpose a))
;#Const(#&lt;FUNCTION (LABELS CL-WAFFE.BACKENDS.MGL::LAZYTRANSPOSE :IN CL-WAFFE.BACKENDS.MGL::LAZY-EVAL-TRANSPOSE) {10038CBADB}&gt;)

(!matmul a (!randn '(3 5)))
;#Const(((0.653... 0.400... 0.471... 0.705... 0.623...)        
;                 ...
;        (1.220... 0.760... 0.975... 1.360... 1.029...)) :mgl t :shape (5 5))
</code></pre>
</div></div>

<h1 id="!transpose1">!transpose1</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!transpose1</code><code class="codex-lambda-list">(x &amp;rest result)</code><div class="codex-docstring"><p>Transpose x but doesn't produce lazy-eval.</p><p>Todo: implement it by myself.</p><h2 id="25-example">Example</h2>
<pre><code class="lisp"></code></pre>
</div></div>

<h1 id="!repeats">!repeats</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!repeats</code><code class="codex-lambda-list">(x axis repeats)</code><div class="codex-docstring">Repeats <code class="codex-param">x</code> along specified <code class="codex-param">axis</code> by <code class="codex-param">repeats</code>, creating new sysconst.<h2 id="26-example">Example</h2>
<pre><code class="lisp">(setq a (!randn '(1 3 3)))
;#Const((((0.333... 0.914... 0.260...)         
;                   ...
;         (0.611... 0.110... 0.113...))) :mgl t :shape (1 3 3))
(!repeats a 0 3)
;#Const((((0.333... 0.914... 0.260...)         
;                   ...
;         (0.611... 0.110... 0.113...))        
;                 ...
;        ((0.333... 0.914... 0.260...)         
;                   ...
;         (0.611... 0.110... 0.113...))) :mgl t :shape (3 3 3))
</code></pre>
</div></div>

<h1 id="!reshape">!reshape</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!reshape</code><code class="codex-lambda-list">(x dim)</code><div class="codex-docstring"><p>Return a new sysconst with changing its shape. x won't be modified.</p><p>If dims has the element of <code class="codex-param">t</code>, t is automatically inferred from the remaining dimensions and the number of elements in dim. (count t dim) must be 1 (Todo: Fix).</p><p>The total size of tensor must not be changed before or after the call to reshape.</p><p>See also: nil</p><h2 id="27-example">Example</h2>
<pre><code class="lisp">(setq a (!randn `(10 10 10)))
(!reshape a '(1 10 100))
;#Const((((0.454... 0.277... ~ 0.536... 0.135...)         
;                   ...
;         (0.857... 0.714... ~ 0.169... 0.279...))) :mgl t :shape (1 10 100))

(!reshape a '(1 1 t))
;#Const((((0.454... 0.277... ~ 0.169... 0.279...))) :mgl t :shape (1 1 1000))
</code></pre>
</div></div>

<h1 id="!abs">!abs</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!abs</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring"><p>Computes the absolute value of each element in <code class="codex-param">x</code>.</p><p>Example:
</p><pre><code class="lisp">(setq a (!random `(10 10) '(-1.0 1.0)))
;#Const(((0.048... 0.805... ~ 0.769... 0.252...)        
;                 ...
;        (0.159... -0.66... ~ -0.55... -0.23...)) :mgl t :shape (10 10))
(!abs a)
;#Const(((0.048... 0.805... ~ 0.769... 0.252...)        
;                 ...
;        (0.159... 0.667... ~ 0.553... 0.239...)) :mgl t :shape (10 10))
</code></pre></div></div>

<h1 id="!where">!where</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!where</code><code class="codex-lambda-list">(condition tensor then else)</code><div class="codex-docstring"><p>Return a tensor of elements selected from either x or y, depending on condition.<code class="codex-param">condition</code> is given as a lambda expression, which called with an value of (aref tensor index).</p><p>!where defined as<code>out = if (condition(tensor[i]), then, else)</code></p><p>Return: A tensor of shape that equal to the condition.</p><h2 id="28-example">Example</h2>
<pre><code class="lisp">(setq a (!random `(10 10) '(-1.0 1.0)))
;#Const(((0.042... -0.36... ~ 0.250... 0.967...)        
;                 ...
;        (-0.21... 0.962... ~ -0.32... 0.215...)) :mgl t :shape (10 10))

(!where #'(lambda (x)(&gt; x 0)) a 1.0 0.0)
;#Const(((1.0 0.0 ~ 1.0 1.0)        
;                 ...
;        (0.0 1.0 ~ 0.0 1.0)) :mgl t :shape (10 10))

; works as ReLU

(!mul a (!where #'(lambda (x)(&gt; x 0)) a 1.0 0.0))
;#Const(((0.042... 0.0... ~ 0.250... 0.967...)        
;                 ...
;        (0.0... 0.962... ~ 0.0... 0.215...)) :mgl t :shape (10 10))
</code></pre>
</div></div>

<h1 id="!index">!index</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!index</code><code class="codex-lambda-list">nil</code><div class="codex-docstring">Todo</div></div>

<h1 id="!filter">!filter</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!filter</code><code class="codex-lambda-list">(tensor lambda)</code><div class="codex-docstring">Applying every tensor's element <code class="codex-param">lambda</code>, it returns an tensor which comprised of the <code class="codex-param">lambda</code>'s returned values.<dl><dt>tensor</dt><dd>an tensor that to be refered to</dd><dt>lambda</dt><dd>an function that returns elements at position <code class="codex-param">x</code></dd></dl>
<pre><code class="lisp">(setq tensor (!randn `(10 10)))
(!filter tensor #'(lambda (x)(if (&gt; x 0) x 1.0)))
;#Const(((0.802... 1.331... ~ 0.998... 1.994...)        
;                 ...
;        (1.0 0.005... ~ 0.296... 0.358...)) :mgl t :shape (10 10))
</code></pre></div></div>

<h1 id="!argmax">!argmax</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!argmax</code><code class="codex-lambda-list">(tensor &amp;key (dim nil) (keepdims nil))</code><div class="codex-docstring"><p>Returns the indices of the maximum value of all elements in the input tensor.</p><dl><dt>dim</dt><dd>The dimension to reduce. If nil, the argmax of the flattened input is returned.</dd><dt>keepdims</dt><dd>whether the output tensor has dim retained or not. Ignored if dim=nil.</dd></dl><h2 id="29-example">Example</h2>
<pre><code class="lisp">(setq a (!randn `(5)))
;#Const((0.933... 0.158... 0.822... 0.881... 0.831...) :mgl t :shape (5))
(!argmax a)
;#Const((0.0) :mgl t :shape (1))
(setq a (!randn `(10 10 10)))
;#Const((((0.393... 0.658... ~ 0.003... 0.609...)         
;                   ...
;         (0.394... 0.252... ~ 0.688... 0.057...))        
;                 ...
;        ((0.325... 0.794... ~ 0.540... 0.381...)         
;                   ...
;         (0.310... 0.035... ~ 0.280... 0.431...))) :mgl t :shape (10 10 10))

(!argmax a :dim 2)

;#Const(((5.0 9.0 ~ 0.0 4.0)        
;                 ...
;        (2.0 0.0 ~ 2.0 5.0)) :mgl t :shape (10 10))

(!argmax a :dim 2 :keepdims t)
;#Const((((5.0 5.0 ~ 5.0 5.0)         
;                   ...
;         (4.0 4.0 ~ 4.0 4.0))        
;                 ...
;        ((2.0 2.0 ~ 2.0 2.0)         
;                   ...
;         (5.0 5.0 ~ 5.0 5.0))) :mgl t :shape (10 10 10))
</code></pre>
</div></div>

<h1 id="!argmin">!argmin</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!argmin</code><code class="codex-lambda-list">(tensor &amp;key (dim nil) (keepdims nil))</code><div class="codex-docstring"><p>Returns the indices of the minimum value of all elements in the input tensor.</p><dl><dt>dim</dt><dd>The dimension to reduce. If nil, the argmax of the flattened input is returned.</dd><dt>keepdims</dt><dd>whether the output tensor has dim retained or not. Ignored if dim=nil.</dd></dl><h2 id="30-example">Example</h2>
<pre><code class="lisp">(setq a (!randn `(5)))
;=&gt;#Const((0.635... 0.101... 0.864... 0.563... 0.481...) :mgl t :shape (5))
(!argmin a)
;=&gt;#Const((1.0) :mgl t :shape (1))

(setq a (!randn `(10 10 10)))
;#Const((((0.267... 0.113... ~ 0.142... 0.208...)         
;                   ...
;         (0.174... 0.948... ~ 0.232... 0.462...))        
;                 ...
;        ((0.454... 0.361... ~ 0.605... 0.731...)         
;                   ...
;         (0.099... 0.816... ~ 0.729... 0.996...))) :mgl t :shape (10 10 10))

(!argmin a)
;#Const((415.0...) :mgl t :shape (1))
</code></pre>
</div></div>

<h1 id="!<=">!&lt;=</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!&lt;=</code><code class="codex-lambda-list">nil</code><div class="codex-docstring">Todo</div></div>

<h1 id="!>=">!&gt;=</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!&gt;=</code><code class="codex-lambda-list">nil</code><div class="codex-docstring">Todo</div></div>

<h1 id="!einsum">!einsum</h1>

<div class="codex-error codex-no-node">No node with name <code>!einsum</code>.</div>

<h1 id="!ravel">!ravel</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!ravel</code><code class="codex-lambda-list">nil</code><div class="codex-docstring">Todo</div></div>

<h1 id="!flatten">!flatten</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!flatten</code><code class="codex-lambda-list">(tensor)</code><div class="codex-docstring"><p>Flattens input by reshaping it into a one-dimensional tensor.</p><p>The operation is the same as <code>(!reshape tensor '(t))</code></p><p>Example:
</p><pre><code class="lisp">(setq a (!randn `(10 10)))
;#Const(((0.688... 0.580... ~ 0.013... 0.461...)        
;                 ...
;        (0.214... 0.248... ~ 0.540... 0.416...)) :mgl t :shape (10 10))

(!flatten a)
;#Const((0.688... 0.580... ~ 0.540... 0.416...) :mgl t :shape (100))
</code></pre></div></div>

<h1 id="!aref">!aref</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!aref</code><code class="codex-lambda-list">(tensor &amp;rest dims)</code><div class="codex-docstring"><p>Very fast aref.</p><p>This function is setfable.</p><p>Cuts the area specified by dims from Tensor and generates a new Const.</p><p>This function creates computation node.</p><p>dims are following:
</p><ol><li>fixnum</li><li>t ... which means 0 ... maxlen</li><li>Cons (e.g. '(1 3) reads 1&lt;=x&lt;3)
</li></ol><p>Example:
</p><pre><code class="lisp">(setq a (!randn `(10 10)))

;=&gt; #Const(((0.280... 1.941... ~ 0.723... -0.47...)        
;                   ...
;          (-1.01... 0.232... ~ -1.16... 0.405...)) :mgl t :shape (10 10))

(!aref a '(0 3) t)
(!aref a t '(0 3))
(!aref a 1 1)
(setq a (setf (!aref a '(0 3) '(0 3))(!zeros '(3 3)))) ; to update nodes

</code></pre></div></div>

<h1 id="!dotensors">!dotensors</h1>


<h1 id="!set-batch">!set-batch</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!set-batch</code><code class="codex-lambda-list">(dataset start-row-index batch-size)</code><div class="codex-docstring"><p>Set batch where dataset is a 2d mat.</p><p>Todo: Backward.</p></div></div>

<h1 id="!softmax">!softmax</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!softmax</code><code class="codex-lambda-list">(x &amp;key (avoid-overflow t))</code><div class="codex-docstring"><p>Applying softmax to x. !softmax has three behaviours depending on the number of dimensions.</p><p>The number of dims is...
</p><dl><dt>1</dt><dd>Softmax is applied to dim=0
<pre><code class="lisp">(setq a (!randn `(10)))
(!softmax a)
;#Const((0.910... 0.886... ~ 0.802... 0.616...) :mgl t :shape (10))
</code></pre>
</dd><dt>2</dt><dd>Softmax is applied to dim=0
<pre><code class="lisp"></code></pre>
</dd><dt>3</dt><dd>Softmax is applied to dim=0
<pre><code class="lisp"></code></pre>
</dd><dt>4</dt><dd>Todo: currently, it returns error.
<pre><code class="lisp"></code></pre>
</dd></dl></div></div>

<h1 id="!sigmoid">!sigmoid</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!sigmoid</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring"><p>Applyong sigmoid to x, return a new sysconst with making nodes.</p><p>Input: x where x is waffe supported data type.</p><p>Output: Tensor</p></div></div>

<h1 id="!relu">!relu</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!relu</code><code class="codex-lambda-list">(x)</code><div class="codex-docstring"><p>Applying relu to x, return a new sysconst with making nodes.</p><p>Relu(x) = { 0 (x &lt; 0), x (x &gt; 0) }</p><p>Input: x where x is waffe supported data type.</p><p>Output: Tensor</p></div></div>

<h1 id="!gelu">!gelu</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!gelu</code><code class="codex-lambda-list">(x &amp;key (approximate t))</code><div class="codex-docstring"><p>Applying gelu to x, returning a new sysconst.</p><p>Paper: https://arxiv.org/abs/1606.08415.</p><p>TOOD: Improve its performance</p><p>GeLU(x) = x * s(x)</p><p>When approximate is t:</p><p>s(x) = x/2 * [1 + tanh(sqrt(2/pi * (x + 0.044715 * x^3)))]</p><p>When is nil:</p><p>Not implemented (TODO)</p><pre><code class="lisp">(setq x (!randn `(10 10)))
(!gelu x)
;#Const(((0.201... 0.038... ~ 0.158... 0.040...)        
;                 ...
;        (0.300... 1.395... ~ 0.030... 0.029...)) :mgl t :shape (10 10))
</code></pre></div></div>

<h1 id="!leakey-relu">!leakey-relu</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!leakey-relu</code><code class="codex-lambda-list">(x &amp;optional (alpha 0.01))</code><div class="codex-docstring"><p>Applying Leakey-relu to x, returning a new sysconst.</p><p>Leakey-ReLU is defined as out = {alpha (x &lt; 0), x (x &gt;= 0)}</p><p>Example:</p><pre><code class="lisp">(setq x (!randn `(10 10)))
#Const(((0.635... -0.56... ~ -1.15... -1.50...)        
                 ...
        (0.775... 1.258... ~ -1.29... 0.240...)) :mgl t :shape (10 10))

(!leakey-relu x)
#Const(((0.635... 0.003... ~ 0.013... 0.022...)        
                 ...
        (0.775... 1.258... ~ 0.016... 0.240...)) :mgl t :shape (10 10))
</code></pre></div></div>

<h1 id="!swish">!swish</h1>

<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">!swish</code><code class="codex-lambda-list">(x &amp;key (beta (const 1.0)))</code><div class="codex-docstring"><p>Applying swish to each element of x</p><p>Swish is defined as out = (/ 1 (+ 1 (exp (* beta -1 x))))</p><p>In default beta is 1.0, if you want to use trainable one, <code class="codex-param">Swish</code> is available as a waffe model.</p><p>Note that beta must begin given as a waffetensor.</p><pre><code class="lisp">(setq x (!randn `(10 10)))
#Const(((0.635... -0.56... ~ -1.15... -1.50...)        
                 ...
        (0.775... 1.258... ~ -1.29... 0.240...)) :mgl t :shape (10 10))

(!swish x)
;#Const(((0.415... -0.20... ~ -0.27... -0.27...)        
;                 ...
;        (0.531... 0.980... ~ -0.27... 0.134...)) :mgl t :shape (10 10))

(call (Swish :beta 1.0) x) ; its beta is trainable by backpropgating.
;#Const(((0.415... -0.20... ~ -0.27... -0.27...)        
;                 ...
;        (0.531... 0.980... ~ -0.27... 0.134...)) :mgl t :shape (10 10))
</code></pre></div></div>


      </div>
    </main>
  </article>
  <footer>
    <div class="info">
      Created with <a href="https://github.com/CommonDoc/codex">Codex</a>.
    </div>
  </footer>
  <script>
   HighlightLisp.highlight_auto();
  </script>

  </body>
</html>
