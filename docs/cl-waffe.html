<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
  cl-waffe &ndash; cl-waffe
</title>
    <link rel="stylesheet" href="static/style.css"/>
    
  <link rel="stylesheet" href="static/highlight.css"/>
  <script src="static/highlight.js"></script>
  <style>
   /* Highlight the current top-level TOC item, and hide the TOC of all other items */

   .toc a[data-node="cl-waffe"] {
       /*color: #AD3108;*/
   }

   .toc ol {
       display: none;
   }

   .toc li a[data-node="cl-waffe"] {
       font-weight: bold;
   }

   .toc li a[data-node="cl-waffe"] + ol {
       display: block;
   }

   .toc li a[data-node="cl-waffe"] + ol li {
       margin-left: 10px;
   }
  </style>

  </head>
  <body>
    
  <h1 class="doc-title">cl-waffe</h1>
  <article id="article" data-section="cl-waffe">
    <aside>
      <ol class="toc"><li><a href="overview.html" data-node="overview">Overview</a><ol><li><a href="overview.html#welcome-to-cl-waffe!" data-node="welcome-to-cl-waffe!">Welcome to cl-waffe!</a></li><li><a href="overview.html#problems" data-node="problems">Problems</a></li><li><a href="overview.html#sections" data-node="sections">Sections</a></li><li><a href="overview.html#pull-requests" data-node="pull-requests">Pull Requests</a></li><li><a href="overview.html#contacts" data-node="contacts">Contacts</a></li><li><a href="overview.html#lla-setting" data-node="lla-setting">LLA Setting</a></li><li><a href="overview.html#when-memory-exhausted" data-node="when-memory-exhausted">When Memory Exhausted</a></li></ol></li><li><a href="mnist-tutorial.html" data-node="mnist-tutorial">MNIST Tutorial</a><ol><li><a href="mnist-tutorial.html#first" data-node="first">First</a></li><li><a href="mnist-tutorial.html#define-your-model" data-node="define-your-model">Define Your Model</a></li><li><a href="mnist-tutorial.html#define-your-dataset" data-node="define-your-dataset">Define Your Dataset</a><ol><li><a href="cl-waffe's-dataset--waffedataset.html" data-node="cl-waffe's-dataset--waffedataset">cl-waffe's Dataset: WaffeDataSet</a></li></ol></li><li><a href="mnist-tutorial.html#train-your-model" data-node="train-your-model">Train Your Model</a></li></ol></li><li><a href="extend-library.html" data-node="extend-library">Extend library</a><ol><li><a href="extend-library.html#exported" data-node="exported">Exported</a></li></ol></li><li><a href="using-tensor.html" data-node="using-tensor">Using Tensor</a><ol><li><a href="using-tensor.html#basic-tensor-operations" data-node="basic-tensor-operations">Basic Tensor Operations</a></li><li><a href="using-tensor.html#tensor" data-node="tensor">Tensor</a><ol><li><a href="using-tensor.html#basic-of-tensor-and-backward" data-node="basic-of-tensor-and-backward">Basic of Tensor and backward</a><ol><li><a href="using-tensor.html#initialize-tensor" data-node="initialize-tensor">Initialize Tensor</a><ol><li><a href="using-tensor.html#parameters" data-node="parameters">Parameters</a></li><li><a href="using-tensor.html#constants" data-node="constants">Constants</a></li><li><a href="using-tensor.html#tensor-vs-const" data-node="tensor-vs-const">Tensor vs Const</a></li></ol></li></ol></li><li><a href="using-tensor.html#forward-nodes" data-node="forward-nodes">Forward Nodes</a></li><li><a href="using-tensor.html#exported-parameters" data-node="exported-parameters">Exported Parameters</a></li><li><a href="using-tensor.html#types" data-node="types">Types</a></li><li><a href="using-tensor.html#accessor" data-node="accessor">Accessor</a></li></ol></li></ol></li><li><a href="cl-waffe.html" data-node="cl-waffe">cl-waffe</a><ol><li><a href="cl-waffe.html#package--cl-waffe" data-node="package--cl-waffe">Package :cl-waffe</a></li><li><a href="cl-waffe.html#0-sections" data-node="0-sections">Sections</a></li><li><a href="cl-waffe.html#model-and-node" data-node="model-and-node">Model And Node</a></li><li><a href="cl-waffe.html#defmodel" data-node="defmodel">defmodel</a></li><li><a href="cl-waffe.html#defnode" data-node="defnode">defnode</a></li><li><a href="cl-waffe.html#trainer" data-node="trainer">Trainer</a></li><li><a href="cl-waffe.html#deftrainer" data-node="deftrainer">deftrainer</a></li><li><a href="cl-waffe.html#datasets" data-node="datasets">Datasets</a></li><li><a href="cl-waffe.html#defdataset" data-node="defdataset">defdataset</a></li><li><a href="cl-waffe.html#optimizer" data-node="optimizer">optimizer</a></li><li><a href="cl-waffe.html#defoptimizer" data-node="defoptimizer">defoptimizer</a></li><li><a href="cl-waffe.html#documentation-template" data-node="documentation-template">Documentation Template</a></li></ol></li><li><a href="cl-waffe.nn.html" data-node="cl-waffe.nn">cl-waffe.nn</a><ol><li><a href="1-exported.html" data-node="1-exported">Exported</a></li></ol></li><li><a href="cl-waffe.optimizers.html" data-node="cl-waffe.optimizers">cl-waffe.optimizers</a><ol><li><a href="2-sections.html" data-node="2-sections">Sections</a></li></ol></li><li><a href="cl-waffe.io.html" data-node="cl-waffe.io">cl-waffe.io</a><ol><li><a href="3-exported.html" data-node="3-exported">Exported</a></li></ol></li><li><a href="cl-waffe.caches.html" data-node="cl-waffe.caches">cl-waffe.caches</a><ol><li><a href="4-exported.html" data-node="4-exported">Exported</a></li></ol></li><li><a href="operators.html" data-node="operators">Operators</a><ol><li><a href="!shape.html" data-node="!shape">!shape</a></li><li><a href="!dims.html" data-node="!dims">!dims</a></li><li><a href="!size.html" data-node="!size">!size</a></li><li><a href="!zeros.html" data-node="!zeros">!zeros</a></li><li><a href="!ones.html" data-node="!ones">!ones</a></li><li><a href="!fill.html" data-node="!fill">!fill</a></li><li><a href="!arange.html" data-node="!arange">!arange</a><ol><li><a href="(-!arange-stop-).html" data-node="(-!arange-stop-)">(!arange stop)</a></li><li><a href="(-!arange-start-stop-).html" data-node="(-!arange-start-stop-)">(!arange start stop)</a></li><li><a href="(-!arange-start-stop-step-).html" data-node="(-!arange-start-stop-step-)">(!arange start stop step)</a></li></ol></li><li><a href="!random.html" data-node="!random">!random</a><ol><li><a href="when-limit=fixnum.html" data-node="when-limit=fixnum">When limit=fixnum</a></li><li><a href="when-limit=single-float.html" data-node="when-limit=single-float">When limit=single-float</a></li><li><a href="when-limit=-(-cons-single-float1-single-float2-).html" data-node="when-limit=-(-cons-single-float1-single-float2-)">When limit=(cons single-float1 single-float2)</a></li></ol></li><li><a href="!random-with.html" data-node="!random-with">!random-with</a></li><li><a href="!init-with.html" data-node="!init-with">!init-with</a></li><li><a href="!normal.html" data-node="!normal">!normal</a></li><li><a href="!randn.html" data-node="!randn">!randn</a></li><li><a href="!beta.html" data-node="!beta">!beta</a></li><li><a href="!gamma.html" data-node="!gamma">!gamma</a></li><li><a href="!chisquare.html" data-node="!chisquare">!chisquare</a></li><li><a href="!bernoulli.html" data-node="!bernoulli">!bernoulli</a></li><li><a href="!binomial.html" data-node="!binomial">!binomial</a></li><li><a href="5-!shape.html" data-node="5-!shape">!shape</a></li><li><a href="6-!dims.html" data-node="6-!dims">!dims</a></li><li><a href="7-!size.html" data-node="7-!size">!size</a></li><li><a href="!zeros-like.html" data-node="!zeros-like">!zeros-like</a></li><li><a href="!ones-like.html" data-node="!ones-like">!ones-like</a></li><li><a href="!full-like.html" data-node="!full-like">!full-like</a></li><li><a href="!add.html" data-node="!add">!add</a><ol><li><a href="examples.html" data-node="examples">Examples</a></li></ol></li><li><a href="!sub.html" data-node="!sub">!sub</a><ol><li><a href="8-examples.html" data-node="8-examples">Examples</a></li></ol></li><li><a href="!mul.html" data-node="!mul">!mul</a><ol><li><a href="9-examples.html" data-node="9-examples">Examples</a></li></ol></li><li><a href="!div.html" data-node="!div">!div</a><ol><li><a href="10-examples.html" data-node="10-examples">Examples</a></li></ol></li><li><a href="!dot.html" data-node="!dot">!dot</a><ol><li><a href="example.html" data-node="example">Example</a></li></ol></li><li><a href="!sum.html" data-node="!sum">!sum</a><ol><li><a href="arguments.html" data-node="arguments">arguments</a></li><li><a href="11-example.html" data-node="11-example">Example</a></li></ol></li><li><a href="!mean.html" data-node="!mean">!mean</a><ol><li><a href="12-example.html" data-node="12-example">Example</a></li></ol></li><li><a href="!exp.html" data-node="!exp">!exp</a><ol><li><a href="13-example.html" data-node="13-example">Example</a></li></ol></li><li><a href="!pow.html" data-node="!pow">!pow</a><ol><li><a href="14-example.html" data-node="14-example">Example</a></li></ol></li><li><a href="!sqrt.html" data-node="!sqrt">!sqrt</a><ol><li><a href="15-example.html" data-node="15-example">Example</a></li></ol></li><li><a href="!log.html" data-node="!log">!log</a><ol><li><a href="16-example.html" data-node="16-example">Example</a></li></ol></li><li><a href="!sin.html" data-node="!sin">!sin</a><ol><li><a href="17-example.html" data-node="17-example">Example</a></li></ol></li><li><a href="!cos.html" data-node="!cos">!cos</a><ol><li><a href="18-example.html" data-node="18-example">Example</a></li></ol></li><li><a href="!tan.html" data-node="!tan">!tan</a><ol><li><a href="19-example.html" data-node="19-example">Example</a></li></ol></li><li><a href="!asin.html" data-node="!asin">!asin</a></li><li><a href="!acos.html" data-node="!acos">!acos</a></li><li><a href="!atan.html" data-node="!atan">!atan</a></li><li><a href="!sinh.html" data-node="!sinh">!sinh</a><ol><li><a href="20-example.html" data-node="20-example">Example</a></li></ol></li><li><a href="!cosh.html" data-node="!cosh">!cosh</a><ol><li><a href="21-example.html" data-node="21-example">Example</a></li></ol></li><li><a href="!tanh.html" data-node="!tanh">!tanh</a></li><li><a href="!asinh.html" data-node="!asinh">!asinh</a></li><li><a href="!acosh.html" data-node="!acosh">!acosh</a></li><li><a href="!atanh.html" data-node="!atanh">!atanh</a></li><li><a href="!matmul.html" data-node="!matmul">!matmul</a></li><li><a href="!unsqueeze.html" data-node="!unsqueeze">!unsqueeze</a><ol><li><a href="22-example.html" data-node="22-example">Example</a></li></ol></li><li><a href="!squeeze.html" data-node="!squeeze">!squeeze</a><ol><li><a href="23-example.html" data-node="23-example">Example</a></li></ol></li><li><a href="!transpose.html" data-node="!transpose">!transpose</a><ol><li><a href="24-example.html" data-node="24-example">Example</a></li></ol></li><li><a href="!transpose1.html" data-node="!transpose1">!transpose1</a><ol><li><a href="25-example.html" data-node="25-example">Example</a></li></ol></li><li><a href="!repeats.html" data-node="!repeats">!repeats</a><ol><li><a href="26-example.html" data-node="26-example">Example</a></li></ol></li><li><a href="!reshape.html" data-node="!reshape">!reshape</a><ol><li><a href="27-example.html" data-node="27-example">Example</a></li></ol></li><li><a href="!abs.html" data-node="!abs">!abs</a></li><li><a href="!where.html" data-node="!where">!where</a><ol><li><a href="28-example.html" data-node="28-example">Example</a></li></ol></li><li><a href="!index.html" data-node="!index">!index</a></li><li><a href="!argmax.html" data-node="!argmax">!argmax</a><ol><li><a href="29-example.html" data-node="29-example">Example</a></li></ol></li><li><a href="!argmin.html" data-node="!argmin">!argmin</a><ol><li><a href="30-example.html" data-node="30-example">Example</a></li></ol></li><li><a href="!<=.html" data-node="!<=">!&lt;=</a></li><li><a href="!>=.html" data-node="!>=">!&gt;=</a></li><li><a href="!einsum.html" data-node="!einsum">!einsum</a></li><li><a href="!ravel.html" data-node="!ravel">!ravel</a></li><li><a href="!flatten.html" data-node="!flatten">!flatten</a></li><li><a href="!aref.html" data-node="!aref">!aref</a></li><li><a href="!dotensors.html" data-node="!dotensors">!dotensors</a></li><li><a href="!set-batch.html" data-node="!set-batch">!set-batch</a></li><li><a href="!softmax.html" data-node="!softmax">!softmax</a></li><li><a href="!sigmoid.html" data-node="!sigmoid">!sigmoid</a></li><li><a href="!relu.html" data-node="!relu">!relu</a></li><li><a href="!gelu.html" data-node="!gelu">!gelu</a></li><li><a href="!leakey-relu.html" data-node="!leakey-relu">!leakey-relu</a></li><li><a href="!swish.html" data-node="!swish">!swish</a></li></ol></li><li><a href="neural-networks.html" data-node="neural-networks">Neural Networks</a><ol><li><a href="model-list.html" data-node="model-list">model-list</a><ol><li><a href="cl-waffe's-model--model-list.html" data-node="cl-waffe's-model--model-list">cl-waffe's Model: model-list</a></li></ol></li><li><a href="linearlayer.html" data-node="linearlayer">Linearlayer</a></li><li><a href="denselayer.html" data-node="denselayer">Denselayer</a></li><li><a href="dropout.html" data-node="dropout">Dropout</a><ol><li><a href="cl-waffe's-node--dropout.html" data-node="cl-waffe's-node--dropout">cl-waffe's Node: Dropout</a></li></ol></li><li><a href="batchnorm2d.html" data-node="batchnorm2d">BatchNorm2d</a><ol><li><a href="cl-waffe's-model--batchnorm2d.html" data-node="cl-waffe's-model--batchnorm2d">cl-waffe's Model: BatchNorm2d</a></li></ol></li><li><a href="layernorm.html" data-node="layernorm">LayerNorm</a></li><li><a href="embedding.html" data-node="embedding">Embedding</a><ol><li><a href="cl-waffe's-model--embedding.html" data-node="cl-waffe's-model--embedding">cl-waffe's Model: Embedding</a></li></ol></li><li><a href="rnn.html" data-node="rnn">RNN</a><ol><li><a href="cl-waffe's-model--rnn.html" data-node="cl-waffe's-model--rnn">cl-waffe's Model: RNN</a></li></ol></li><li><a href="lstm.html" data-node="lstm">LSTM</a></li><li><a href="gru.html" data-node="gru">GRU</a></li><li><a href="maxpooling.html" data-node="maxpooling">MaxPooling</a></li><li><a href="avgpooling.html" data-node="avgpooling">AvgPooling</a></li><li><a href="conv1d.html" data-node="conv1d">Conv1D</a></li><li><a href="conv2d.html" data-node="conv2d">Conv2D</a></li><li><a href="transformer.html" data-node="transformer">Transformer</a></li><li><a href="transformerencoderlayer.html" data-node="transformerencoderlayer">TransformerEncoderLayer</a></li><li><a href="transformerdecoderlayer.html" data-node="transformerdecoderlayer">TransformerDecoderLayer</a></li><li><a href="crossentropy.html" data-node="crossentropy">CrossEntropy</a></li><li><a href="softmaxcrossentropy.html" data-node="softmaxcrossentropy">SoftMaxCrossEntropy</a></li><li><a href="mse.html" data-node="mse">MSE</a></li><li><a href="l1norm.html" data-node="l1norm">L1Norm</a></li><li><a href="l2norm.html" data-node="l2norm">L2Norm</a></li><li><a href="binarycrossentropy.html" data-node="binarycrossentropy">BinaryCrossEntropy</a></li><li><a href="kldivloss.html" data-node="kldivloss">KLdivLoss</a></li><li><a href="cosinesimilarity.html" data-node="cosinesimilarity">CosineSimilarity</a></li></ol></li><li><a href="optimizers.html" data-node="optimizers">Optimizers</a><ol><li><a href="sgd.html" data-node="sgd">SGD</a><ol><li><a href="cl-waffe's-optimizer--sgd.html" data-node="cl-waffe's-optimizer--sgd">cl-waffe's Optimizer: SGD</a></li></ol></li><li><a href="momentum.html" data-node="momentum">Momentum</a><ol><li><a href="cl-waffe's-optimizer--momentum.html" data-node="cl-waffe's-optimizer--momentum">cl-waffe's Optimizer: Momentum</a></li></ol></li><li><a href="adagrad.html" data-node="adagrad">AdaGrad</a><ol><li><a href="cl-waffe's-optimizer--adagrad.html" data-node="cl-waffe's-optimizer--adagrad">cl-waffe's Optimizer: AdaGrad</a></li></ol></li><li><a href="rmsprop.html" data-node="rmsprop">RMSProp</a><ol><li><a href="cl-waffe's-optimizer--rmsprop.html" data-node="cl-waffe's-optimizer--rmsprop">cl-waffe's Optimizer: RMSProp</a></li></ol></li><li><a href="adam.html" data-node="adam">Adam</a><ol><li><a href="cl-waffe's-optimizer--adam.html" data-node="cl-waffe's-optimizer--adam">cl-waffe's Optimizer: Adam</a></li></ol></li><li><a href="adamw.html" data-node="adamw">AdamW</a></li><li><a href="radam.html" data-node="radam">RAdam</a></li></ol></li></ol>
    </aside>
    <main class="codex-section">
      <header>
        <h2 class="section-title">cl-waffe</h2>
      </header>
      <div class="content">
        <p>
</p><h1 id="package--cl-waffe">Package :cl-waffe</h1><p>
⚠️This package is under development and APIs can be changed without notice.</p><p>This package has:
</p><ol><li>WaffeTensor</li><li>Basic Tensor Operators</li><li>Utils for define extensions (e.g.: defmodel)</li><li>Utils for training and validating (e.g.: deftrainer)</li></ol><p>
</p><h1 id="0-sections">Sections</h1><dl><dt>Four Arithmetic Operations</dt><dd><p><a href="./operators.html#!add">!add</a> Applying two tensors +</p><p>
<a href="./operators.html#!sub">!sub</a> Applying two tensors -</p><p>
<a href="./operators.html#!mul">!mul</a> Applying two tensors *</p><p>
<a href="./operators.html#!div">!div</a>   Applying two tensors /
</p></dd><dt>Sum up and obtain a average</dt><dd><p><a href="./operators.html#!sum">!sum</a>    Sum up the given tensor in specified dims, and if needed, repeat it.</p><p>
<a href="./operators.html#!mean">!mean</a> Find the average of a specified dimension.
</p></dd><dt>Multiplying matrices</dt><dd><p><a href="./operators.html#!dot">!dot</a>        Returns the dot product of two tensors which are 1D.</p><p>
<a href="./operators.html#!matmul">!matmul</a>  Multiplying matrices x and y. The returned value depends on the dimension of x and y.</p><p>
<a href="./operators.html#!einsum">!einsum</a>
</p></dd><dt>Shaping</dt><dd><p><a href="./operators.html#!squeeze">!squeeze</a> Returns a new tensor with a dimension of size one removed at the specified position.</p><p>
<a href="./operators.html#!unsqueeze">!unsqueeze</a> Returns a new tensor with a dimension of size one inserted at the specified position.</p><p>
<a href="./operators.html#!transpose">!transpose</a> transpose a tensor for !matmul</p><p>
<a href="./operators.html#!transpose1">!transpose1</a> transpose a tensor but doens't produce an lazy-evaluated tensor.</p><p>
<a href="./operators.html#!reshape">!reshape</a></p><p>
<a href="./operators.html#!repeats">!repeats</a></p><p>
<a href="./operators.html#!flatten">!flatten</a></p><p>
<a href="./operators.html#!ravel">!ravel</a></p><p>
<a href="./operators.html#!shape">!shape</a></p><p>
<a href="./operators.html#!dims">!dims</a></p><p>
<a href="./operators.html#!size">!size</a> 
</p></dd><dt>Trigonometric Functions</dt><dd><p><a href="./operators.html#!sin">!sin</a></p><p>
<a href="./operators.html#!cos">!cos</a></p><p>
<a href="./operators.html#!tan">!tan</a></p><p>
<a href="./operators.html#!asin">!asin</a></p><p>
<a href="./operators.html#!acos">!acos</a></p><p>
<a href="./operators.html#!atan">!atan</a></p><p>
<a href="./operators.html#!sinh">!sinh</a></p><p>
<a href="./operators.html#!cosh">!cosh</a></p><p>
<a href="./operators.html#!tanh">!tanh</a></p><p>
<a href="./operators.html#!asinh">!asinh</a></p><p>
<a href="./operators.html#!acosh">!acosh</a></p><p>
<a href="./operators.html#!atanh">!atanh</a>
</p></dd><dt>Math Functions</dt><dd><p><a href="./operators.html#!abs">!abs</a></p><p>
<a href="./operators.html#!log">!log</a></p><p>
<a href="./operators.html#!exp">!exp</a></p><p>
<a href="./operators.html#!pow">!pow</a></p><p>
<a href="./operators.html#!sqrt">!sqrt</a>
</p></dd><dt>Activations</dt><dd><p><a href="./operators.html#!tanh">!tanh</a></p><p>
<a href="./operators.html#!sigmoid">!sigmoid</a></p><p>
<a href="./operators.html#!relu">!relu</a></p><p>
<a href="./operators.html#!gelu">!gelu</a></p><p>
<a href="./operators.html#!leakey-relu">!leakey-relu</a></p><p>
<a href="./operators.html#!swish">!swish</a></p><p>
<a href="./operators.html#!softmax">!softmax</a>
</p></dd><dt>Handling Multidimensional Tensors</dt><dd><p><a href="./operators.html#!aref">!aref</a></p><p>
<a href="./operators.html#!dotensors">!dotensors</a></p><p>
<a href="./operators.html#!set-batch">!set-batch</a></p><p>
<a href="./operators.html#!where">!where</a></p><p>
<a href="./operators.html#!index">!index</a>
</p></dd><dt>Initialize A Tensor With Specified Elements</dt><dd><p><a href="./operators.html#!zeros">!zeros</a></p><p>
<a href="./operators.html#!zeros">!ones</a></p><p>
<a href="./operators.html#!zeros">!fill</a> fill a tensor with specified value</p><p>
<a href="./operators.html#!zeros">!zeros-like</a></p><p>
<a href="./operators.html#!zeros">!ones-like</a></p><p>
<a href="./operators.html#!zeros">!full-like</a></p><p>
<a href="./operators.html#!arange">!arange</a></p><p>
<a href="./operators.html#!init-with">!init-with</a>
</p></dd><dt>Random</dt><dd><p><a href="./operators.html#!random">!random</a></p><p>
<a href="./operators.html#!random-with">!random-with</a></p><p>
<a href="./operators.html#!normal">!normal</a></p><p>
<a href="./operators.html#!randn">!randn</a></p></dd><dt>Samplign Probability Distributions</dt><dd><p><a href="./operators.html#!randn">!randn</a></p><p>
<a href="./operators.html#!beta">!beta</a></p><p>
<a href="./operators.html#!gamma">!gamma</a></p><p>
<a href="./operators.html#!chisquare">!chisquare</a></p><p>
<a href="./operators.html#!bernoulli">!bernoulli</a>
</p></dd><dt>The usage of Model And Node</dt><dd><p><a href="./cl-waffe.html#model-and-node">Model And Node</a></p><p>
<a href="./cl-waffe.html#defmodel">Define Model And Use it</a></p><p>
<a href="./cl-waffe.html#defnode">Define Node And Use it</a>
</p></dd><dt>The usage of Trainers</dt><dd><p><a href="./cl-waffe.html#trainer">Trainer</a></p><p>
<a href="./cl-waffe.html#deftrainer">Define Trainer</a>
</p></dd><dt>The usage of Optimizers</dt><dd><p><a href="./cl-waffe.html#optimizer">Optimizer</a></p><p>
<a href="./cl-waffe.html#defoptimizer">Define Optimizer</a>
</p></dd><dt>The usage of Datasets</dt><dd><p><a href="./cl-waffe.html#datasets">Datasets</a></p><p>
<a href="./cl-waffe.html#defdataset">Define Datasets</a>
</p></dd><dt>Describe Docstrign in the template</dt><dd><a href="./cl-waffe.html#documentation-template">with-usage</a>
</dd></dl><p>
</p><h1 id="model-and-node">Model And Node</h1><p>Todo: tutorial</p><h1 id="defmodel">defmodel</h1>
<p><div class="codex-doc-node codex-operator codex-macro"><code class="codex-name">defmodel</code><code class="codex-lambda-list">(name args &amp;key (parameters nil) forward (optimize nil) (document An model, defined by cl-waffe))</code><div class="codex-docstring"><p>This macro defines a cl-waffe model as <code class="codex-param">name</code>.</p><p>At the same time, a constructor <code class="codex-param">name</code> is defined and you can initialize your model like:</p><pre><code class="lisp">(cl-waffe.nn:LinearLayer 100 20) ; =&gt; [Model: Linearlayer]
</code></pre><p>
</p><dl><dt>name</dt><dd>Your model and constructor name</dd><dt>args</dt><dd>The arguments of a constructor</dd><dt>parameters</dt><dd><p>The parameters your model has.</p><p>Every time you initialize the model, the parameters are initialized.</p><p>Note that <code class="codex-param">defmodel</code> behaves like class.</p><p>The arguments are the same as <a href="http://l1sp.org/cl/defstruct"><code>defstruct</code></a></p><p>Format Example: ((param-name param-initial-value &amp;key (type your-type)))</p></dd><dt>optimize</dt><dd>when t, your forward slot is defined with (declare (optimize (speed 3)(space 0)(debug 0))). It helps faster training after you ensured debugged.</dd><dt>forward</dt><dd><p>Define here the forward propagation of your model.</p><p>When backward, <b>Automatic differentiation applies</b>.</p></dd></dl><p>
</p></div></div>
<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">call</code><code class="codex-lambda-list">(model &amp;rest args)</code><div class="codex-docstring"><p>Calls the forward steps which defined in: defnode, defmodel, defoptimizer.</p><p>All forward steps must be called through this function, otherwise the returned tensor doesn't have: computation nodes, thread-datum which supports performance.</p><p>Building computation nodes is ignored when *no-grad* is t.</p><dl><dt>model</dt><dd>Your initialized model/node/optimizer objects</dd><dt>args</dt><dd>Arguments :forward needs</dd></dl><p>Example:
</p><pre><code class="lisp">(defnode Add nil
  :optimize t
  :parameters nil
  :forward  ((x y)
	     (+ x y))
  :backward ((dy)(list dy dy)))

(call (Add)(const 1.0)(const 1.0))
;=&gt;Const(2.0)

</code></pre><p>Output: Waffetensor of list which comprised of waffetensor.</p></div></div>
<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">backward</code><code class="codex-lambda-list">(tensor)</code><div class="codex-docstring"><p>Compute back propagation by traversing the Tensor's computation node.</p><p>The parameters of the model defined by (tensor) or to which (Parameter tensor) is applied, store the gradient in grad slot.</p><p>Note that: tensor must be the shape of `(1) or single value. Otherwise an error occurs.</p><p>In the process calculating backward, new backwards won't be created. (*no-grad* automatically becomes t)</p><dl><dt>Input</dt><dd>WaffeTensor</dd><dt>Output</dt><dd>NIL</dd></dl></div></div>
<div class="codex-doc-node codex-operator codex-macro"><code class="codex-name">with-calling-layers</code><code class="codex-lambda-list">(input &amp;rest layers)</code><div class="codex-docstring"><p>This macro allows to sequentially call layers.</p><p>the argument <code class="codex-param">input</code> must be a tensor.</p><p>Refering each layers from (self) macro, destructively modifying x with the returned value.</p><p>Note: This macro supposes models to be returned a single tensor, not a list.</p><pre><code class="lisp">(defmodel MLP (activation)
   :parameters ((layer1   (denselayer (* 28 28) 512 T activation))
   	        (layer2   (denselayer 512 256 T activation))
	        (layer3   (linearlayer 256 10 T)))
   :forward ((x)
	     (with-calling-layers x
	       (layer1 x)
 	       (layer2 x)
               (layer3 x))))
</code></pre><p>For the different arguments.</p><pre><code class="lisp">(with-calling-layers x
     (layer1 x 1 1)
     (layer2 1 x 2)
     (layer3 x y))
</code></pre><p>Output: An last value of layers.</p></div></div>
</p>
<h1 id="defnode">defnode</h1>
<p><div class="codex-doc-node codex-operator codex-macro"><code class="codex-name">defnode</code><code class="codex-lambda-list">(name args &amp;key parameters forward backward optimize (regard-as-node t) (document An node, defined by cl-waffe.))</code><div class="codex-docstring"><p>Defining computation nodes.</p><p>defnode is useful when you want to define the derivative yourself.<b>Note that parameter tensors in :parameter won't updated by optimizers.</b></p><p>If you want to update params, define additional models.</p><dl><dt>regard-as-node</dt><dd>When the slot :regard-as-node is nil, an optimizer in cl-waffe.caches regards this as model (i.e. argument could be destructed.) Default is t.
</dd></dl><p>Note that:</p><ol><li>:backward must return list, where that length corresponds with the length of input's argument, otherwise an error occurs when backward.</li><li>In forward and backward, computation node isn't needed to be continuous.However, the last values of :forward and :backward step, must posses :thread-data, which can be obtained by (waffetensor-thread-data tensor)</li></ol><p>Example:</p><pre><code class="lisp">(defnode AddTensor nil
  :optimize t
  :parameters nil
  :forward  ((x y)
	     (with-searching-calc-node :add x y))
  :backward ((dy)(list dy dy)))

(call (AddTensor) tensor1 tensor2)
</code></pre></div></div>
<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">call</code><code class="codex-lambda-list">(model &amp;rest args)</code><div class="codex-docstring"><p>Calls the forward steps which defined in: defnode, defmodel, defoptimizer.</p><p>All forward steps must be called through this function, otherwise the returned tensor doesn't have: computation nodes, thread-datum which supports performance.</p><p>Building computation nodes is ignored when *no-grad* is t.</p><dl><dt>model</dt><dd>Your initialized model/node/optimizer objects</dd><dt>args</dt><dd>Arguments :forward needs</dd></dl><p>Example:
</p><pre><code class="lisp">(defnode Add nil
  :optimize t
  :parameters nil
  :forward  ((x y)
	     (+ x y))
  :backward ((dy)(list dy dy)))

(call (Add)(const 1.0)(const 1.0))
;=&gt;Const(2.0)

</code></pre><p>Output: Waffetensor of list which comprised of waffetensor.</p></div></div>
<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">backward</code><code class="codex-lambda-list">(tensor)</code><div class="codex-docstring"><p>Compute back propagation by traversing the Tensor's computation node.</p><p>The parameters of the model defined by (tensor) or to which (Parameter tensor) is applied, store the gradient in grad slot.</p><p>Note that: tensor must be the shape of `(1) or single value. Otherwise an error occurs.</p><p>In the process calculating backward, new backwards won't be created. (*no-grad* automatically becomes t)</p><dl><dt>Input</dt><dd>WaffeTensor</dd><dt>Output</dt><dd>NIL</dd></dl></div></div></p><p><div class="codex-doc-node codex-operator codex-function"><code class="codex-name">warranty</code><code class="codex-lambda-list">(tensor)</code><div class="codex-docstring"><p>Notice waffe's optimizer that do not delete tensor given until warranty called
   in the calc node.</p><p> When you encountered error in the forward step that the tensor that attempted to read has already cached ~, try this like:</p><pre><code class="lisp">(warranty your-tensor)
(print your-tensor)
</code></pre></div></div>
<div class="codex-doc-node codex-operator codex-macro"><code class="codex-name">with-kernel-case</code><code class="codex-lambda-list">(target var &amp;key (mgl nil) (mgl-cuda nil) (copy t) &amp;aux (out (gensym)))</code><div class="codex-docstring"><p>Reading the target's device, this macro invokes property codes described in :mgl, :mgl-cuda etc...</p><p>   Dynamically defining and caching cpu and cuda kernel.</p><p>   Every time reaches this macro, cl-waffe caches the target (i.e. the target is allowed to be destructed).</p><p>   This macro won't create computation nodes.</p><p>   The available slot is in *kernels*</p><p>   When :mgl-cuda is nil, automatically calls :mgl</p><p>   This macro returns the last value of called slots.</p><p>   The last value of :mgl, :mgl-cuda and so on, must be type of list (cons), or mgl-mat:mat, waffetensorcontenttype.</p><p>   Note: the target's thread-data must be already created. (i.e. By the time tensors reach this macro, at least once they needed to be pathed through Trainer or Model.)
   So, use this macro when you defining :forward and :backward in defnode macro because in defnode, backprop is disabled and computation nodes isn't always required.</p><p>Inputs
</p><dl><dt>target</dt><dd>an target tensor.</dd><dt>var</dt><dd>where an copied tensor of target will be assigned.</dd><dt>:mgl</dt><dd>mgl-mat, when using cpu.</dd><dt>:mgl-mat</dt><dd>mgl-mat, when using cuda.</dd></dl><p>Return: An tensor (where tensor is made by sysconst)</p><p>Example:</p><pre><code class="lisp">(with-kernel-case x o
     :mgl (progn
            (axpy! 1.0 a o)) ; axpy! = !add
     :mgl-cuda nil) =&gt; #Const(((0.0 1.0 ~ 2.0 3.0)        
                 ...
      (0.0 4.0 ~ 5.0 6.0)) :mgl t :shape (10 10))

 ; This is useful when defining :backward
 (with-kernel-case x o
     :mgl (progn
            (list 1 1)))
</code></pre></div></div>
<div class="codex-doc-node codex-operator codex-macro"><code class="codex-name">call-and-dispatch-kernel</code><code class="codex-lambda-list">(kernel-function &amp;rest args)</code><div class="codex-docstring">Invoke kernel and run kernel-function. return new sysconst
It's the most general way for users to access cl-waffe's kernel.
Todo:More Details</div></div></p>
<h1 id="trainer">Trainer</h1><p>Todo: tutorials</p><h1 id="deftrainer">deftrainer</h1><p><p><div class="codex-doc-node codex-operator codex-macro"><code class="codex-name">deftrainer</code><code class="codex-lambda-list">(name args &amp;key model optimizer optimizer-args step-model predict (document An trainer structure defined by cl-waffe.))</code><div class="codex-docstring"><p>Defining trainer, which is made in order to call <code class="codex-param">train</code> function.</p><p>The slots you defined can be invoked by using <code>(step-model model &amp;rest args)</code>, <code>(predict model &amp;rest args)</code>. See below.</p><p>
</p><dl><dt>model</dt><dd>An model defined by <code>(defmodel)</code> which you want to train.</dd><dt>optimizer</dt><dd>An optimizer defined by <code>(defoptimizer)</code></dd><dt>optimizer-args</dt><dd>An arguments for optimizer</dd><dt>step-model</dt><dd>For each batch step, :step-model is called in <code>(train)</code> function. Describe here forward step, backward, zero-grad, update for training.</dd><dt>predict</dt><dd>an code for predicting</dd></dl><p>
These macro below are defined by <a href="http://l1sp.org/cl/macrolet"><code>macrolet</code></a> and you can use them in :step-model, :predict</p><dl><dt>(self name)</dt><dd>access trainer's parameters.</dd><dt>(model)</dt><dd>access trainer's model, defined by :model keyword.</dd><dt>(zero-grad)</dt><dd>Find model's all parameters and constants, and initialize their grads. (i.e. call optimizer's backward)</dd><dt>(update)</dt><dd>Find model's all parameters, and call optimizer and change parameter's data. (i.e. call optimizer's forward)</dd></dl><p>This trainer macro is defined in order to integrate following works:</p><ol><li>calling models</li><li>calling criterions</li><li>calling backward</li><li>calling optimizer</li><li>calling zero-grad</li><li>defining predict</li></ol><p>Example:</p><pre><code class="lisp">(deftrainer MLPTrainer (activation lr)
  :model          (MLP activation)
  :optimizer      cl-waffe.optimizers:Adam ; Note: :optimizer requires a single variable.
  :optimizer-args (:lr lr) ; these arguments directly expanded to optimizer's args.
  :step-model ((x y)
	       (zero-grad) ; call zero-grad
	       (let ((out (cl-waffe.nn:softmax-cross-entropy (call (model) x) y))) ; get criterion
		 (backward out) ; backward
		 (update) ; call optimizer
		 out)) ; return loss
 :predict ((x)(call (model) x))) ;for predict

(setq trainer (MLPTrainer :relu 1e-4)) ; init your trainer

; Train:   (step-model trainer model-input-x model-input-y)
; Predict: (predict trainer model-input-x)

</code></pre></div></div>
<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">step-model</code><code class="codex-lambda-list">(trainer &amp;rest args)</code><div class="codex-docstring"><p>An function for calling trainer object defined by deftrainer
By using this function, trainer's step-model will be invoked.</p><p>Input: Trainer, Args</p></div></div>
<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">predict</code><code class="codex-lambda-list">(trainer &amp;rest args)</code><div class="codex-docstring">An function for calling trainer's predict slot</div></div></p><p><div class="codex-doc-node codex-operator codex-function"><code class="codex-name">train</code><code class="codex-lambda-list">(trainer dataset &amp;key (valid-dataset nil) (valid-each 100) (enable-animation t) (epoch 1) (batch-size 1) (max-iterate nil) (verbose t) (stream t) (progress-bar-freq 1) (save-model-path nil) (width 45) (random nil) (height 10) (print-each 10))</code><div class="codex-docstring"><p>Trainining given trainer. If any, valid <code class="codex-param">valid-dataset</code></p><dl><dt>trainer</dt><dd>Trainer you defined by deftrainer</dd><dt>dataset</dt><dd>Dataset you defined by defdataset</dd><dt>valid-dataset</dt><dd>If valid-dataset=your dataset, use this to valid. If nil, ignored</dd><dt>enable-animation</dt><dd>Ignored</dd><dt>epoch</dt><dd>Iterate training by epoch, default=1</dd><dt>batch-size</dt><dd>Do batch training. default=1</dd><dt>verbose</dt><dd>if t, put log to stream</dd></dl><p>This function is temporary and other arguments are ignored.</p><p>And this function has a lot of todo.</p></div></div>
<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">valid</code><code class="codex-lambda-list">(trainer dataset batch-size)</code><div class="codex-docstring">Valid trainer</div></div>
</p></p><h1 id="datasets">Datasets</h1><p>Todo: tutorials here</p><h1 id="defdataset">defdataset</h1><p><p><div class="codex-doc-node codex-operator codex-macro"><code class="codex-name">defdataset</code><code class="codex-lambda-list">(name args &amp;key parameters next length (document An dataset structure defined by cl-waffe.))</code><div class="codex-docstring"><p>Defining dataset. (This is kinda pytorch's dataloader)</p><p>The slots you defined can be invoked by using (get-dataset dataset index)(get-length dataset).</p><dl><dt>parameters</dt><dd>parameters datasets have.</dd><dt>next</dt><dd>when function (get-dataset dataset index) is called, this slot invokes. Return waffetensor for the next batch in response to your task.</dd><dt>length</dt><dd>In this form, the function must return the total length of your datasets where the value is fixnum. (Not a batch, and not a current index.)</dd></dl><pre><code class="lisp">(defdataset Mnistdata (train valid batch-size)
  :parameters ((train train)(valid valid)(batch-size batch-size))
  :next    ((index)
	    (list (!set-batch (self train) index (self batch-size))
		  (!set-batch (self valid) index (self batch-size))))
  :length (()(car (!shape (self train)))))

</code></pre><p>cl-waffe excepts index to be 1, 2, 3, ... (dataset-maxlen)</p><p>So, please manage batch-sizes in args and :next slots.</p></div></div>
<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">get-dataset</code><code class="codex-lambda-list">(dataset index)</code><div class="codex-docstring">Get datum of the index from dataset.
Input: dataset ... dataset defined by defdataset.
       index ... fixnum</div></div>
<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">get-dataset-length</code><code class="codex-lambda-list">(dataset)</code><div class="codex-docstring">Get total size of your dataset.</div></div></p></p><h1 id="optimizer">optimizer</h1>
Tutorials here
<h1 id="defoptimizer">defoptimizer</h1>

<div class="codex-doc-node codex-operator codex-macro"><code class="codex-name">defoptimizer</code><code class="codex-lambda-list">(name args &amp;key parameters update optimize (document An optimizer, defined by cl-waffe.))</code><div class="codex-docstring"><p>Defining optimizers. Internally, This is paraphase of defmodel, which slot names are just different.</p><p>Note: by calling :backward slot, optimizers work as (zero-grad).</p><dl><dt>Name</dt><dd>The optimizer's structure and constructor will be defined based on name</dd><dt>Args</dt><dd>Initializer of the optimizer. The first value of initializer is the hash-table that collected model's parameter where the key is fixnum from 0 to n. You have to store it.</dd><dt>parameters</dt><dd>An parameters that it has.</dd><dt>update</dt><dd>when training and (update) is called, this slot is called and you optimizer your parameters.</dd><dt>optimize</dt><dd>when t, the :update slot is defined with (optimize (speed 3)(space 0)(debug 0)) Default: nil</dd><dt>document</dt><dd>docstring for optimizers. You can use string or (with-usage) macro</dd></dl><p>Example:</p><pre><code class="lisp">;defoptimizer's args must start with params (symbol-name doesn't matter) which receives hash-table whose key is 1..n

(defoptimizer SGD (params &amp;key (lr 1e-3))
  :optimize t
  :parameters ((params params :type hash-table)
               (lr lr :type single-float))
  :update (()
       (dotimes (i (hash-table-count (self params)))
         ; W(n+1) = W(n) - n * grad
         (!modify (gethash i (self params))) :+=
               (!mul (self lr)(grad (gethash i (self params)))))))

</code></pre><p>
</p></div></div>

<h1 id="documentation-template">Documentation Template</h1>

<div class="codex-doc-node codex-operator codex-macro"><code class="codex-name">with-usage</code><code class="codex-lambda-list">(object-name &amp;key (overview Nothing) (args describe like &amp;rest this) (forward Nothing) (step-args describe like &amp;rest this) (backward Nothing) (update Nothing) (step-model Nothing) (predict Nothing) (next Nothing) (length Nothing) (note ))</code><div class="codex-docstring"><p>In :document slot, (for `defmodel, defnode, defoptimizer deftrainer defdataset`) this macro will be useful.</p><p>Keyword:
   step-args, arguments for :forward or :step-model, :next.</p><p>cl-waffe automatically generate docstrings.</p><p>Todo:Write Docs</p></div></div>
<div class="codex-doc-node codex-operator codex-function"><code class="codex-name">build-docstring</code><code class="codex-lambda-list">(usage object-type)</code><div class="codex-docstring"><p>Build docstring based on usage and object-type</p><p>Todo: Write Doc</p></div></div>

<p>
</p>
      </div>
    </main>
  </article>
  <footer>
    <div class="info">
      Created with <a href="https://github.com/CommonDoc/codex">Codex</a>.
    </div>
  </footer>
  <script>
   HighlightLisp.highlight_auto();
  </script>

  </body>
</html>
